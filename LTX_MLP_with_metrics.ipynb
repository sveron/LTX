{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cb33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "from scipy import sparse\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c524e08d-8212-499c-8517-9c0f287ff729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a2c4eb-91a5-4852-b8d4-a8e23311e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'jaccard_based_sim_ML1.pkl'\n",
    "\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(jaccard_based_sim, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    jaccard_based_sim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7222810-6617-42cb-9794-b3b194fd981e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'items_values_dict_ML1.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(items_values_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    items_values_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d69ab8-fcc7-4985-9ffc-c4edc9404fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'user_similarities_Jaccard_ML1.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(user_similarities_Jaccard, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    user_similarities_Jaccard = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ada5339-cb46-407d-851b-f208738eb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'cosine_items_ML1.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(cosine_items_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    cosine_items_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3e4efa-1c12-4045-8eb4-43fb9047dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = cosine_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e027c20-6f3d-4bfb-96ea-64ded575f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'tf_idf_items_ML1.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(cosine_items_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    tf_idf_items = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450a0368-41f8-4501-9e32-059966c3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'pop_dict.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    popularity_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e820bac2-ab4f-4650-9a72-76d50d67aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'shap_values.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    shap_values = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2999317e-1195-4f25-a7b9-7a5348e15a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'item_to_cluster_ML1.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db2d314-127e-4a85-aecf-7ee1571e05c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1208, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29de5b25-1cdd-498f-9f32-ac9602cbf7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5ac75b-5892-4227-9a30-7048e3c43b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ipynb\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "'''\n",
    "distance_to_proximity(distances_list) - takes distances from origin user and returns proximity\n",
    "LimeBase() - class that gets kernel function\n",
    "get_lime_args(user_vetor, item_id, model, items_array, min_pert = 10, max_pert = 20, num_of_perturbations = 5, seed = 0) - \n",
    "    returns neighborhood_data, neighborhood_labels, distances, item_id \n",
    "'''\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1666c-8f99-4cf9-bfb7-85417eeaa0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479c204f-a9cb-4d97-bccd-a536e4192411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5e7530-31df-4896-a82f-ca5cbf513e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.linear_y = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_representation = self.linear_x(user.float())\n",
    "        item_representation = self.linear_y(item.float())\n",
    "        dot_prod = torch.matmul(user_representation, item_representation.T)\n",
    "        dot_sigmoid = self.sigmoid(dot_prod)\n",
    "        \n",
    "        return dot_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041f1d25-56b4-413f-9676-f249d709d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender_G(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size):\n",
    "        super(Recommender_G, self).__init__()\n",
    "        self.mlp = MLP_G(num_items, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, user_vector, item_vector):\n",
    "        user_vector = user_vector.to(device)\n",
    "        item_vector = item_vector.to(device)\n",
    "        output = self.mlp(user_vector, item_vector)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffa3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Explainer_G(nn.Module):\n",
    "    def __init__(self, recommender_model_g, input_size, hidden_size):\n",
    "        super(Explainer_G, self).__init__()\n",
    "        \n",
    "        backbone_children = list(recommender_model_g.children())[0]\n",
    "\n",
    "        self.slice1 = nn.Sequential(*list(backbone_children.children())[:1])\n",
    "        self.slice2 = nn.Sequential(*list(backbone_children.children())[1:2])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=input_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        slice1_output = self.slice1(user.float())\n",
    "        slice2_output = self.slice2(item.float())\n",
    "        combined_output = torch.cat((slice1_output, slice2_output), dim=-1)\n",
    "        mask = self.bottleneck(combined_output).to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb926432-c8ef-4b77-8666-e914ba3ad651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ee527d7-301d-4f84-a2a7-3729f4af99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class LossModelCombined(torch.nn.Module):\n",
    "    def __init__(self,alpha_parameter, recommender_model, explainer_model, hidden_size):\n",
    "     \n",
    "        super().__init__()\n",
    "            \n",
    "        self.recommender_model =  recommender_model\n",
    "        self.explainer_model= explainer_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_parameter = alpha_parameter\n",
    "        #self.beta_parameter = beta_parameter\n",
    "        \n",
    "    def forward(self,user_hist, y_positive, y_negative=None):\n",
    "      \n",
    "        user_hist = user_hist.to(device)\n",
    "        y_positive = torch.tensor(y_positive).float().to(device)\n",
    "        \n",
    "        mask = self.explainer_model(user_hist,y_positive).to(device)\n",
    "        x_masked = user_hist * mask\n",
    "        y_positive_masked = self.recommender_model(x_masked,y_positive).unsqueeze(0).to(device)      \n",
    "        pred_loss = -torch.log(y_positive_masked).to(device)\n",
    "        \n",
    "        pred_negative_loss = 0\n",
    "        if y_negative is not None:\n",
    "            y_negative_masked = self.recommender_model(x_masked,y_negative).unsqueeze(0).to(device) \n",
    "            pred_negative_loss = torch.log(y_negative_masked).to(device)\n",
    "\n",
    "        \n",
    "        mask_loss = torch.mean(torch.abs(mask)).to(device)\n",
    "   \n",
    "        comb_loss = pred_loss + self.alpha_parameter * mask_loss # + self.beta_parameter*pred_negative_loss \n",
    "        \n",
    "        #print(\"Pred loss: {:.4f}, L1 loss: {:.4f}, Negative Loss {:.4f}\".format(pred_loss.item(), mask_loss.item(), pred_negative_loss.item()))\n",
    "        \n",
    "        return x_masked, comb_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b390ca-eec4-4998-8286-bb745b9d93b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be59aebc-807c-41d0-8ea3-bfec4683898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train and test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a96e4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users is  6040\n",
      "num_items is  3706\n"
     ]
    }
   ],
   "source": [
    "# Train the model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "hidden_dim = 20\n",
    "num_users = 6040\n",
    "num_items = 3706\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37cab73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model = Recommender_G(num_items, hidden_dim)\n",
    "rec_model.load_state_dict(torch.load('recommender_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd3b70a-bb23-4a27-a5a6-6f45e604e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in rec_model.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c19ed0c5-c088-4c13-ba61-2d59b4cda1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a95439-e2de-4bfb-aec4-46d7274b7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed = pd.read_csv('train_data_mixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f451f90-03f1-4207-ab58-ff1ee7ebd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dad84345-6533-4680-9dd8-1b14137fd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('items_values_dict_ML1.pkl', 'rb') as f:\n",
    "    items_values_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2920eb41-d5cf-40e2-b46d-393eaab30d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('prob_dict.pkl', 'rb') as f:\n",
    "    prob_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5492be4d-77a2-4027-bf9c-02a59b4e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_values= pd.read_csv('items_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bfc5b9b-dd0b-4eca-9e86-3ad934735248",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a10384-2767-45f2-9f38-149755c9b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "253a1f53-6f41-4fea-a2a2-ffb619d1d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top k dictionaries for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a154033f-5fed-41a0-88f7-26de7d2e664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_train.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47f1600-5173-425b-b909-5ce178903677",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_test.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53ffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524e4244-be51-407b-8526-31e761b4b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e1b2bed-e049-43af-972f-26c13e700e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k(user_vector, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    item_tensor = torch.FloatTensor(items_array).to(device)\n",
    "    output_model = [float(i) for i in model(user_tensor, item_tensor).cpu().detach().numpy()]\n",
    "    \n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        item_prob_dict[i]=output[i]\n",
    "\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ddae8-d089-4ee3-ac92-4b4f24cdf51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecf100df-077d-4140-bc76-695ea4ce8fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_in_the_list(user_vector, original_user_vector, item_id, num_items, model):\n",
    "    top_k_list = list(get_top_k(user_vector, original_user_vector, num_items, model, num_items).keys())\n",
    "    return top_k_list.index(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "195a9425-d6fa-4b50-948c-6bb7ff2e96ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5addf221-c6e9-49d1-9c2f-c0776c2aff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ndcg(ranked_list, target_item):\n",
    "    if target_item not in ranked_list:\n",
    "        return 0.0\n",
    "\n",
    "    target_idx = torch.tensor(ranked_list.index(target_item), device=device)\n",
    "    ndcg = torch.reciprocal(torch.log2(target_idx + 2))\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "988248e6-ac2d-405a-94ec-bd7ca2761841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, recommender_model, model_combined, y_pred = None, mask_type = None, user_id = None):\n",
    "\n",
    "    \n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "    \n",
    "    item_vector = items_values_dict[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    \n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(np.sum(user_vector)), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    rankA_at_1 = [0]*(len(bins))\n",
    "    rankA_at_5 = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    rankB = [0]*(len(bins))\n",
    "    \n",
    "    NDCG_at_1 = [0]*(len(bins))\n",
    "    NDCG_at_5 = [0]*(len(bins))\n",
    "    NDCG_at_10 = [0]*(len(bins))\n",
    "    NDCG_at_20 = [0]*(len(bins))\n",
    "    NDCG_at_50 = [0]*(len(bins))\n",
    "    NDCG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    total_items = 0\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            if mask_type == 'jaccard_g':\n",
    "                sim_items = find_jaccard_g_mask(POS_masked, item_id, num_items, jaccard_based_sim , num_samples=bins[i])\n",
    "            elif mask_type == 'jaccard_u':\n",
    "                sim_items = find_jaccard_u_mask(POS_masked, item_id, num_items, user_similarities_Jaccard , num_samples=bins[i])\n",
    "            elif mask_type == 'cosine':\n",
    "                sim_items = find_cosine_mask(POS_masked, item_id, num_items, cosine_items , num_samples=bins[i])\n",
    "            elif mask_type == 'tf_idf':\n",
    "                sim_items = find_tf_idf_mask(POS_masked, item_id, num_items, tf_idf_items)\n",
    "            elif mask_type == 'pop':\n",
    "                sim_items = find_pop_mask(POS_masked, item_id, num_items , num_samples=bins[i])\n",
    "            elif mask_type == 'shap': \n",
    "                sim_items = find_shapley_mask(POS_masked, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "            elif mask_type == 'lime':\n",
    "                sim_items = find_LIME_mask(user_vector, item_id, items_array, 50, 100, 150, distance_to_proximity,'highest_weights', model = recommender_model, num_samples=user_hist_size)\n",
    "            elif mask_type == 'ltx':\n",
    "                sim_items = find_LTX_mask(POS_masked, item_vector, item_id, model_combined)\n",
    "            else:\n",
    "                raise Exception(\"Wrong mask type\")\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "        \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked\n",
    "        \n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "\n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor,num_items, recommender_model,num_items)        \n",
    "        POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        NEG_index = get_index_in_the_list(NEG_masked,user_tensor, item_id, num_items, recommender_model)+1\n",
    "   \n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_model(POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_model(user_tensor-POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "       # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1+1-POS_index)/10)\n",
    "        rankA_at_5[i] = max(0, (5+1-POS_index)/20)\n",
    "        rankA_at_10[i] = max(0, (10+1-POS_index)/10)\n",
    "        rankA_at_20[i] = max(0, (20+1-POS_index)/20)\n",
    "        rankA_at_50[i] = max(0, (50+1-POS_index)/50)\n",
    "        rankA_at_100[i] = max(0, (100+1-POS_index)/100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1/POS_index\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG_at_1[i]= get_ndcg(list(POS_ranked_list.keys())[:1],item_id)\n",
    "        NDCG_at_5[i]= get_ndcg(list(POS_ranked_list.keys())[:5],item_id)\n",
    "        NDCG_at_10[i]= get_ndcg(list(POS_ranked_list.keys())[:10],item_id)\n",
    "        NDCG_at_20[i]= get_ndcg(list(POS_ranked_list.keys())[:20],item_id)\n",
    "        NDCG_at_50[i]= get_ndcg(list(POS_ranked_list.keys())[:50],item_id)\n",
    "        NDCG_at_100[i]= get_ndcg(list(POS_ranked_list.keys())[:100],item_id)\n",
    "\n",
    "    res = [POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100, DEL, INS, rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100, rankB, NDCG_at_1, NDCG_at_5, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b5698-ca28-435a-90d7-fe1c7293c5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7796d852-24e2-45a1-af3f-87290c718cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8d42338-5589-4575-9d2e-9f19d91b004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LIME_mask(x, item_id, items_array, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, model, num_samples=10, method = 'POS'):\n",
    "    \n",
    "    user_hist = x \n",
    "    # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = mlp_get_lime_args(user_hist, item_id, model, items_array, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id)\n",
    "  \n",
    "    most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    most_pop_items_dict =  {key: value for key, value in most_pop_items}\n",
    "    \n",
    "    return most_pop_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5f8b5d6-617e-4ff1-bdac-7ad826053bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX based similarity\n",
    "def find_LTX_mask(x, y_true, item_id, model_combined):\n",
    "    \n",
    "    user_hist = torch.tensor(x) \n",
    "    user_hist[item_id] = 0 \n",
    "\n",
    "    x_masked_g, loss_comb_g = model_combined(user_hist, y_true)\n",
    "    \n",
    "    x_masked_g = x_masked_g.to(device)\n",
    "    #item_sim_dict = {i: v for i, v in enumerate(x_masked_l)}\n",
    "    item_sim_dict = {i: x_masked_g[i].item() for i in range(x_masked_g.numel())}\n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "617c4c4a-de4c-42e5-8572-3acf8ad93355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Genre based similarities using Jaccard\n",
    "def find_jaccard_g_mask(x, item_id, num_items,  jaccard_based_sim,num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in jaccard_based_sim.keys()):\n",
    "                item_jaccard_dict[i]=jaccard_based_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "    \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01a902eb-5acc-40b9-a170-c7025aee3b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_u_mask(x, item_id, num_items, user_based_Jaccard_sim,num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in user_based_Jaccard_sim.keys()):\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "            \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ad6767b-0c7b-49de-8a9e-625ddb771891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, num_items, item_cosine, num_samples=10, method = 'POS'):\n",
    "\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "        \n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in item_cosine.keys()):\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)] # add cosine similarity between items\n",
    "            else:\n",
    "                item_cosine_dict[i] = 0\n",
    "    \n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23b55846-3cdf-440d-8915-24097bd0e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf mask\n",
    "def find_tf_idf_mask(x, item_id, num_items, tf_idf_sim):\n",
    "    \n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "    \n",
    "    positive_items = np.where(x == 1)[0]\n",
    "    tf_idf_dict = {i: tf_idf_sim.get((i, item_id), 0) for i in positive_items}\n",
    "    \n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "960f4701-a38b-485d-bc34-942d4fe5e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#popularity mask\n",
    "def find_pop_mask(x, item_id, num_items, num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate over all positive items of the user\n",
    "            item_pop_dict[i]=popularity_dict[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10c16852-3069-4037-9afc-fec28cefab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_vector, user_id, model, shap_values, item_to_cluster):\n",
    "        \n",
    "        item_shap = {}\n",
    "        shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,2:]\n",
    "        user_vector = user_vector.cpu().detach().numpy().astype(int)\n",
    "        \n",
    "        for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "            items_cluster = item_to_cluster[i]\n",
    "            item_shap[i] = shapley_values.T[int(items_cluster)][0]\n",
    "  \n",
    "        return item_shap     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8f86e27-a3f6-46ff-a2fa-7e3742e827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8a26a-9b74-4f29-8cb4-f67750e39dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "075a4a8b-04b6-4391-a70a-bce22d900344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get users vectors to create topk\n",
    "unique_indices = np.unique(train_array[:,-3], return_index=True, axis=0)[1]\n",
    "\n",
    "# create a new array with only the unique users\n",
    "train_unique_arr = train_array[unique_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe054c3b-9c29-4f77-82f5-06ffba6dc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_model_g = Explainer_G(rec_model, num_items, hidden_dim).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb999b3f-c39c-4cb1-b2e5-32387d242246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.1002\n",
      "Epoch 1, Train Loss 0.0640\n",
      "Epoch 2, Train Loss 0.0503\n",
      "Epoch 3, Train Loss 0.0433\n",
      "Epoch 4, Train Loss 0.0375\n",
      "Epoch 5, Train Loss 0.0331\n",
      "Epoch 6, Train Loss 0.0358\n",
      "Epoch 7, Train Loss 0.0312\n",
      "Epoch 8, Train Loss 0.0298\n",
      "Epoch 9, Train Loss 0.0298\n"
     ]
    }
   ],
   "source": [
    "#LTX training\n",
    "\n",
    "import time\n",
    "import random\n",
    "train_losses = []\n",
    "epochs = 10\n",
    "hidden_dim = 10\n",
    "num_of_bins = 10\n",
    "alpha_parameter = 0.5\n",
    "#beta_parameter = 0.2\n",
    "\n",
    "model_combined = LossModelCombined(alpha_parameter, rec_model, explainer_model_g, hidden_dim).to(device)\n",
    "optimizer_comb = torch.optim.Adam(model_combined.parameters(), lr=0.04)\n",
    "    \n",
    "#Train LTX\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(train_unique_arr.shape[0]):\n",
    "        \n",
    "        #user data\n",
    "        user_id = train_unique_arr[i][-3]\n",
    "        user_vector = train_unique_arr[i][:-3]\n",
    "        #get top1 of this user for LTX training\n",
    "        top1_item = np.argmax(topk_train[user_id])\n",
    "        #print(\"top1_item \", top1_item)\n",
    "        #Get negative item\n",
    "        #top_indices = np.argsort(list(topk_train[user_id]))[-100:-1]\n",
    "        # Sample one index randomly\n",
    "        #top_100_indices = np.delete(top_indices, np.where(top_indices == top1_item))\n",
    "    \n",
    "        # sample an index from the top 100\n",
    "        #not_top1 = np.random.choice(top_100_indices)\n",
    "        #print(not_top1)\n",
    "        #print(\"not_top1_item \", not_top1)\n",
    "       \n",
    "        positive_sample = torch.FloatTensor(items_values_dict[top1_item]).to(device)\n",
    "        #negative_sample = torch.FloatTensor(items_values_dict[not_top1]).to(device)\n",
    "        \n",
    "        user_vector[top1_item] = 0 \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        #train model        \n",
    "        optimizer_comb.zero_grad()\n",
    "        x_masked_g, loss_comb_g = model_combined(user_tensor, positive_sample)#, negative_sample)\n",
    "        \n",
    "        train_loss+=loss_comb_g.item()\n",
    "            \n",
    "        loss_comb_g.backward()\n",
    "        optimizer_comb.step()\n",
    "   \n",
    "    train_losses.append(train_loss/train_unique_arr.shape[0])\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss {train_loss/train_unique_arr.shape[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e6490426-daf5-401f-8bde-f767660afb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model_combined.state_dict(), 'mlp_model_combined_10_epochs_05.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "19ab9629-7c3a-4f52-9779-4940bd3d7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(explainer_model_g.state_dict(), 'mlp_explainer_model_10_epochs_05.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7459b47-e7da-4d2a-ba08-bd7f10c2d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX on testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "934651c9-7d21-4009-87db-488a922b622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "POS_at_1_j_g:  0.0\n",
      "POS_at_5_j_g:  0.0\n",
      "POS_at_10_j_g:  0.0\n",
      "POS_at_20_j_g:  0.0\n",
      "POS_at_50_j_g:  0.0\n",
      "POS_at_100_j_g:  0.0\n",
      "NEG_at_1_j_g:  0.0\n",
      "NEG_at_5_j_g:  0.0\n",
      "NEG_at_10_j_g:  0.0\n",
      "NEG_at_20_j_g:  0.0\n",
      "NEG_at_50_j_g:  0.0\n",
      "NEG_at_100_j_g:  0.0\n",
      "users_DEL_j_g:  0.0\n",
      "users_INS_j_g:  0.0\n",
      "rank_at_1_j_g:  0.0\n",
      "rank_at_5_j_g:  0.0\n",
      "rank_at_10_A_j_g:  0.0\n",
      "rank_at_20_A_j_g:  0.0\n",
      "rank_at_50_A_j_g:  0.0\n",
      "rank_at_100_A_j_g:  0.0\n",
      "rank_at_k_B_j_g:  0.0\n",
      "NDCG_at_1_j_g:  0.0\n",
      "NDCG_at_5_j_g:  0.0\n",
      "NDCG_at_10_j_g:  0.0\n",
      "NDCG_at_20_j_g:  0.0\n",
      "NDCG_at_50_j_g:  0.0\n",
      "NDCG_at_100_j_g:  0.0\n",
      "POS_at_1_ltx:  0.0\n",
      "POS_at_5_ltx:  0.0\n",
      "POS_at_10_ltx:  0.0\n",
      "POS_at_20_ltx:  0.0\n",
      "POS_at_50_ltx:  0.0\n",
      "POS_at_100_ltx:  0.0\n",
      "NEG_at_1_ltx:  0.0\n",
      "NEG_at_5_ltx:  0.0\n",
      "NEG_at_10_ltx:  0.0\n",
      "NEG_at_20_ltx:  0.0\n",
      "NEG_at_50_ltx:  0.0\n",
      "NEG_at_100_ltx:  0.0\n",
      "users_DEL_ltx:  0.0\n",
      "users_INS_ltx:  0.0\n",
      "rank_at_1_ltx:  0.0\n",
      "rank_at_5_ltx:  0.0\n",
      "rank_at_10_A_ltx:  0.0\n",
      "rank_at_20_A_ltx:  0.0\n",
      "rank_at_50_A_ltx:  0.0\n",
      "rank_at_100_A_ltx:  0.0\n",
      "rank_at_k_B_ltx:  0.0\n",
      "NDCG_at_1_ltx:  0.0\n",
      "NDCG_at_5_ltx:  0.0\n",
      "NDCG_at_10_ltx:  0.0\n",
      "NDCG_at_20_ltx:  0.0\n",
      "NDCG_at_50_ltx:  0.0\n",
      "NDCG_at_100_ltx:  0.0\n",
      "POS_at_1_shap:  0.24710264900662252\n",
      "POS_at_5_shap:  0.35587748344370856\n",
      "POS_at_10_shap:  0.4147350993377483\n",
      "POS_at_20_shap:  0.4709437086092715\n",
      "POS_at_50_shap:  0.5795529801324504\n",
      "POS_at_100_shap:  0.6733443708609271\n",
      "NEG_at_1_shap:  0.22996688741721855\n",
      "NEG_at_5_shap:  0.3348509933774834\n",
      "NEG_at_10_shap:  0.38874172185430467\n",
      "NEG_at_20_shap:  0.4432119205298013\n",
      "NEG_at_50_shap:  0.5507450331125827\n",
      "NEG_at_100_shap:  0.6437086092715232\n",
      "users_DEL_shap:  0.8846360638735905\n",
      "users_INS_shap:  0.9343646848311186\n",
      "rank_at_1_shap:  0.024710264900662332\n",
      "rank_at_5_shap:  0.07820364238410596\n",
      "rank_at_10_A_shap:  0.3541804635761585\n",
      "rank_at_20_A_shap:  0.4008443708609273\n",
      "rank_at_50_A_shap:  0.4800463576158946\n",
      "rank_at_100_A_shap:  0.5565082781456964\n",
      "rank_at_k_B_shap:  0.3058915282956655\n",
      "NDCG_at_1_shap:  0.24710264900662252\n",
      "NDCG_at_5_shap:  0.3051170776666\n",
      "NDCG_at_10_shap:  0.3242669452721907\n",
      "NDCG_at_20_shap:  0.35986574613520045\n",
      "NDCG_at_100_shap:  0.37509433127174907\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Evaluate the model on the test set\n",
    "num_of_bins = 11\n",
    "k = 100\n",
    "\n",
    "\n",
    "POS_at_1_j_g = np.zeros(num_of_bins)\n",
    "POS_at_5_j_g = np.zeros(num_of_bins)\n",
    "POS_at_10_j_g = np.zeros(num_of_bins)\n",
    "POS_at_20_j_g = np.zeros(num_of_bins)\n",
    "POS_at_50_j_g = np.zeros(num_of_bins)\n",
    "POS_at_100_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_g = np.zeros(num_of_bins)\n",
    "users_DEL_j_g = np.zeros(num_of_bins)\n",
    "users_INS_j_g = np.zeros(num_of_bins)\n",
    "rank_at_1_j_g = np.zeros(num_of_bins)\n",
    "rank_at_5_j_g = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_g = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_ltx = np.zeros(num_of_bins)\n",
    "POS_at_5_ltx = np.zeros(num_of_bins)\n",
    "POS_at_10_ltx = np.zeros(num_of_bins)\n",
    "POS_at_20_ltx = np.zeros(num_of_bins)\n",
    "POS_at_50_ltx = np.zeros(num_of_bins)\n",
    "POS_at_100_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_1_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_5_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_100_ltx = np.zeros(num_of_bins)\n",
    "users_DEL_ltx = np.zeros(num_of_bins)\n",
    "users_INS_ltx = np.zeros(num_of_bins)\n",
    "rank_at_1_ltx = np.zeros(num_of_bins)\n",
    "rank_at_5_ltx = np.zeros(num_of_bins)\n",
    "rank_at_10_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_20_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_50_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_100_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_k_B_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_1_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_5_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_100_ltx = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_shap = np.zeros(num_of_bins)\n",
    "POS_at_5_shap = np.zeros(num_of_bins)\n",
    "POS_at_10_shap = np.zeros(num_of_bins)\n",
    "POS_at_20_shap = np.zeros(num_of_bins)\n",
    "POS_at_50_shap = np.zeros(num_of_bins)\n",
    "POS_at_100_shap = np.zeros(num_of_bins)\n",
    "NEG_at_1_shap = np.zeros(num_of_bins)\n",
    "NEG_at_5_shap = np.zeros(num_of_bins)\n",
    "NEG_at_10_shap = np.zeros(num_of_bins)\n",
    "NEG_at_20_shap = np.zeros(num_of_bins)\n",
    "NEG_at_50_shap = np.zeros(num_of_bins)\n",
    "NEG_at_100_shap = np.zeros(num_of_bins)\n",
    "users_DEL_shap = np.zeros(num_of_bins)\n",
    "users_INS_shap = np.zeros(num_of_bins)\n",
    "rank_at_1_shap = np.zeros(num_of_bins)\n",
    "rank_at_5_shap = np.zeros(num_of_bins)\n",
    "rank_at_10_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_20_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_50_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_100_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_k_B_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_1_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_5_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_10_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_20_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_50_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_100_shap = np.zeros(num_of_bins)\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "explainer_model_g.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        #item_id = test_array[i][-1]\n",
    "        user_id = test_array[i][-2]\n",
    "\n",
    "        #item_vector = items_values.iloc[item_id]\n",
    "        user_vector = test_array[i][:-2]\n",
    "\n",
    "        y_pred = topk_test[user_id]\n",
    "        item_id = np.argmax(y_pred)\n",
    "\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "     \n",
    "       \n",
    "\n",
    "         ### Lime:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, user_id = user_id, mask_type= 'shap')    \n",
    "        POS_at_1_shap += res[0]\n",
    "        POS_at_5_shap += res[1]\n",
    "        POS_at_10_shap += res[2]\n",
    "        POS_at_20_shap += res[3]\n",
    "        POS_at_50_shap += res[4]\n",
    "        POS_at_100_shap += res[5]\n",
    "        NEG_at_1_shap += res[6]\n",
    "        NEG_at_5_shap += res[7]\n",
    "        NEG_at_10_shap += res[8]\n",
    "        NEG_at_20_shap += res[9]\n",
    "        NEG_at_50_shap += res[10]\n",
    "        NEG_at_100_shap += res[11]\n",
    "        users_DEL_shap += res[12]\n",
    "        users_INS_shap += res[13]\n",
    "        rank_at_1_shap += res[14]\n",
    "        rank_at_5_shap += res[15]\n",
    "        rank_at_10_A_shap += res[16]\n",
    "        rank_at_20_A_shap += res[17]\n",
    "        rank_at_50_A_shap += res[18]\n",
    "        rank_at_100_A_shap += res[19]\n",
    "        rank_at_k_B_shap += res[20]\n",
    "        NDCG_at_1_shap += res[21]\n",
    "        NDCG_at_5_shap += res[22]\n",
    "        NDCG_at_10_shap += res[23]\n",
    "        NDCG_at_20_shap += res[24]\n",
    "        NDCG_at_50_shap += res[25]\n",
    "        NDCG_at_100_shap += res[26]\n",
    "        \n",
    "      \n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "           \n",
    "a = i+1\n",
    "\n",
    "print('POS_at_1_j_g: ', np.mean(POS_at_1_j_g[1:])/a)\n",
    "print('POS_at_5_j_g: ', np.mean(POS_at_5_j_g[1:])/a)\n",
    "print('POS_at_10_j_g: ', np.mean(POS_at_10_j_g[1:])/a)\n",
    "print('POS_at_20_j_g: ', np.mean(POS_at_20_j_g[1:])/a)\n",
    "print('POS_at_50_j_g: ', np.mean(POS_at_50_j_g[1:])/a)\n",
    "print('POS_at_100_j_g: ', np.mean(POS_at_100_j_g[1:])/a)\n",
    "print('NEG_at_1_j_g: ', np.mean(NEG_at_1_j_g[1:])/a)\n",
    "print('NEG_at_5_j_g: ', np.mean(NEG_at_5_j_g[1:])/a)\n",
    "print('NEG_at_10_j_g: ', np.mean(NEG_at_10_j_g[1:])/a)\n",
    "print('NEG_at_20_j_g: ', np.mean(NEG_at_20_j_g[1:])/a)\n",
    "print('NEG_at_50_j_g: ', np.mean(NEG_at_50_j_g[1:])/a)\n",
    "print('NEG_at_100_j_g: ', np.mean(NEG_at_100_j_g[1:])/a)\n",
    "print('users_DEL_j_g: ', np.mean(users_DEL_j_g[1:])/a)\n",
    "print('users_INS_j_g: ', np.mean(users_INS_j_g[1:])/a)\n",
    "print('rank_at_1_j_g: ', np.mean(rank_at_1_j_g[1:])/a)\n",
    "print('rank_at_5_j_g: ', np.mean(rank_at_5_j_g[1:])/a)\n",
    "print('rank_at_10_A_j_g: ', np.mean(rank_at_10_A_j_g[1:])/a)\n",
    "print('rank_at_20_A_j_g: ', np.mean(rank_at_20_A_j_g[1:])/a)\n",
    "print('rank_at_50_A_j_g: ', np.mean(rank_at_50_A_j_g[1:])/a)\n",
    "print('rank_at_100_A_j_g: ', np.mean(rank_at_100_A_j_g[1:])/a)\n",
    "print('rank_at_k_B_j_g: ', np.mean(rank_at_k_B_j_g[1:])/a)\n",
    "print('NDCG_at_1_j_g: ', np.mean(NDCG_at_1_j_g[1:])/a)\n",
    "print('NDCG_at_5_j_g: ', np.mean(NDCG_at_5_j_g[1:])/a)\n",
    "print('NDCG_at_10_j_g: ', np.mean(NDCG_at_10_j_g[1:])/a)\n",
    "print('NDCG_at_20_j_g: ', np.mean(NDCG_at_20_j_g[1:])/a)\n",
    "print('NDCG_at_50_j_g: ', np.mean(NDCG_at_50_j_g[1:])/a)\n",
    "print('NDCG_at_100_j_g: ', np.mean(NDCG_at_100_j_g[1:])/a)\n",
    "\n",
    "print('POS_at_1_ltx: ', np.mean(POS_at_1_ltx[1:])/a)\n",
    "print('POS_at_5_ltx: ', np.mean(POS_at_5_ltx[1:])/a)\n",
    "print('POS_at_10_ltx: ', np.mean(POS_at_10_ltx[1:])/a)\n",
    "print('POS_at_20_ltx: ', np.mean(POS_at_20_ltx[1:])/a)\n",
    "print('POS_at_50_ltx: ', np.mean(POS_at_50_ltx[1:])/a)\n",
    "print('POS_at_100_ltx: ', np.mean(POS_at_100_ltx[1:])/a)\n",
    "print('NEG_at_1_ltx: ', np.mean(NEG_at_1_ltx[1:])/a)\n",
    "print('NEG_at_5_ltx: ', np.mean(NEG_at_5_ltx[1:])/a)\n",
    "print('NEG_at_10_ltx: ', np.mean(NEG_at_10_ltx[1:])/a)\n",
    "print('NEG_at_20_ltx: ', np.mean(NEG_at_20_ltx[1:])/a)\n",
    "print('NEG_at_50_ltx: ', np.mean(NEG_at_50_ltx[1:])/a)\n",
    "print('NEG_at_100_ltx: ', np.mean(NEG_at_100_ltx[1:])/a)\n",
    "print('users_DEL_ltx: ', np.mean(users_DEL_ltx[1:])/a)\n",
    "print('users_INS_ltx: ', np.mean(users_INS_ltx[1:])/a)\n",
    "print('rank_at_1_ltx: ', np.mean(rank_at_1_ltx[1:])/a)\n",
    "print('rank_at_5_ltx: ', np.mean(rank_at_5_ltx[1:])/a)\n",
    "print('rank_at_10_A_ltx: ', np.mean(rank_at_10_A_ltx[1:])/a)\n",
    "print('rank_at_20_A_ltx: ', np.mean(rank_at_20_A_ltx[1:])/a)\n",
    "print('rank_at_50_A_ltx: ', np.mean(rank_at_50_A_ltx[1:])/a)\n",
    "print('rank_at_100_A_ltx: ', np.mean(rank_at_100_A_ltx[1:])/a)\n",
    "print('rank_at_k_B_ltx: ', np.mean(rank_at_k_B_ltx[1:])/a)\n",
    "print('NDCG_at_1_ltx: ', np.mean(NDCG_at_1_ltx[1:])/a)\n",
    "print('NDCG_at_5_ltx: ', np.mean(NDCG_at_5_ltx[1:])/a)\n",
    "print('NDCG_at_10_ltx: ', np.mean(NDCG_at_10_ltx[1:])/a)\n",
    "print('NDCG_at_20_ltx: ', np.mean(NDCG_at_20_ltx[1:])/a)\n",
    "print('NDCG_at_50_ltx: ', np.mean(NDCG_at_50_ltx[1:])/a)\n",
    "print('NDCG_at_100_ltx: ', np.mean(NDCG_at_100_ltx[1:])/a)\n",
    "\n",
    "print('POS_at_1_shap: ', np.mean(POS_at_1_shap[1:])/a)\n",
    "print('POS_at_5_shap: ', np.mean(POS_at_5_shap[1:])/a)\n",
    "print('POS_at_10_shap: ', np.mean(POS_at_10_shap[1:])/a)\n",
    "print('POS_at_20_shap: ', np.mean(POS_at_20_shap[1:])/a)\n",
    "print('POS_at_50_shap: ', np.mean(POS_at_50_shap[1:])/a)\n",
    "print('POS_at_100_shap: ', np.mean(POS_at_100_shap[1:])/a)\n",
    "print('NEG_at_1_shap: ', np.mean(NEG_at_1_shap[1:])/a)\n",
    "print('NEG_at_5_shap: ', np.mean(NEG_at_5_shap[1:])/a)\n",
    "print('NEG_at_10_shap: ', np.mean(NEG_at_10_shap[1:])/a)\n",
    "print('NEG_at_20_shap: ', np.mean(NEG_at_20_shap[1:])/a)\n",
    "print('NEG_at_50_shap: ', np.mean(NEG_at_50_shap[1:])/a)\n",
    "print('NEG_at_100_shap: ', np.mean(NEG_at_100_shap[1:])/a)\n",
    "print('users_DEL_shap: ', np.mean(users_DEL_shap[1:])/a)\n",
    "print('users_INS_shap: ', np.mean(users_INS_shap[1:])/a)\n",
    "print('rank_at_1_shap: ', np.mean(rank_at_1_shap[1:])/a)\n",
    "print('rank_at_5_shap: ', np.mean(rank_at_5_shap[1:])/a)\n",
    "print('rank_at_10_A_shap: ', np.mean(rank_at_10_A_shap[1:])/a)\n",
    "print('rank_at_20_A_shap: ', np.mean(rank_at_20_A_shap[1:])/a)\n",
    "print('rank_at_50_A_shap: ', np.mean(rank_at_50_A_shap[1:])/a)\n",
    "print('rank_at_100_A_shap: ', np.mean(rank_at_100_A_shap[1:])/a)\n",
    "print('rank_at_k_B_shap: ', np.mean(rank_at_k_B_shap[1:])/a)\n",
    "print('NDCG_at_1_shap: ', np.mean(NDCG_at_1_shap[1:])/a)\n",
    "print('NDCG_at_5_shap: ', np.mean(NDCG_at_5_shap[1:])/a)\n",
    "print('NDCG_at_10_shap: ', np.mean(NDCG_at_10_shap[1:])/a)\n",
    "print('NDCG_at_20_shap: ', np.mean(NDCG_at_50_shap[1:])/a)\n",
    "print('NDCG_at_100_shap: ', np.mean(NDCG_at_100_shap[1:])/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ee8b798-dbc2-4a68-930a-3cb999a882ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_at_1_shap:  0.24710264900662252\n",
      "POS_at_5_shap:  0.35587748344370856\n",
      "POS_at_10_shap:  0.4147350993377483\n",
      "POS_at_20_shap:  0.4709437086092715\n",
      "POS_at_50_shap:  0.5795529801324504\n",
      "POS_at_100_shap:  0.6733443708609271\n",
      "NEG_at_1_shap:  0.22996688741721855\n",
      "NEG_at_5_shap:  0.3348509933774834\n",
      "NEG_at_10_shap:  0.38874172185430467\n",
      "NEG_at_20_shap:  0.4432119205298013\n",
      "NEG_at_50_shap:  0.5507450331125827\n",
      "NEG_at_100_shap:  0.6437086092715232\n",
      "users_DEL_shap:  0.8846360638735905\n",
      "users_INS_shap:  0.9343646848311186\n",
      "rank_at_1_shap:  0.024710264900662332\n",
      "rank_at_5_shap:  0.07820364238410596\n",
      "rank_at_10_A_shap:  0.3541804635761585\n",
      "rank_at_20_A_shap:  0.4008443708609273\n",
      "rank_at_50_A_shap:  0.4800463576158946\n",
      "rank_at_100_A_shap:  0.5565082781456964\n",
      "rank_at_k_B_shap:  0.3058915282956655\n",
      "NDCG_at_1_shap:  0.24710264900662252\n",
      "NDCG_at_5_shap:  0.3051170776666\n",
      "NDCG_at_10_shap:  0.3242669452721907\n",
      "NDCG_at_20_shap:  0.3383976044688399\n",
      "NDCG_at_50_shap:  0.35986574613520045\n",
      "NDCG_at_100_shap:  0.37509433127174907\n"
     ]
    }
   ],
   "source": [
    "print('POS_at_1_shap: ', np.mean(POS_at_1_shap[1:])/a)\n",
    "print('POS_at_5_shap: ', np.mean(POS_at_5_shap[1:])/a)\n",
    "print('POS_at_10_shap: ', np.mean(POS_at_10_shap[1:])/a)\n",
    "print('POS_at_20_shap: ', np.mean(POS_at_20_shap[1:])/a)\n",
    "print('POS_at_50_shap: ', np.mean(POS_at_50_shap[1:])/a)\n",
    "print('POS_at_100_shap: ', np.mean(POS_at_100_shap[1:])/a)\n",
    "print('NEG_at_1_shap: ', np.mean(NEG_at_1_shap[1:])/a)\n",
    "print('NEG_at_5_shap: ', np.mean(NEG_at_5_shap[1:])/a)\n",
    "print('NEG_at_10_shap: ', np.mean(NEG_at_10_shap[1:])/a)\n",
    "print('NEG_at_20_shap: ', np.mean(NEG_at_20_shap[1:])/a)\n",
    "print('NEG_at_50_shap: ', np.mean(NEG_at_50_shap[1:])/a)\n",
    "print('NEG_at_100_shap: ', np.mean(NEG_at_100_shap[1:])/a)\n",
    "print('users_DEL_shap: ', np.mean(users_DEL_shap[1:])/a)\n",
    "print('users_INS_shap: ', np.mean(users_INS_shap[1:])/a)\n",
    "print('rank_at_1_shap: ', np.mean(rank_at_1_shap[1:])/a)\n",
    "print('rank_at_5_shap: ', np.mean(rank_at_5_shap[1:])/a)\n",
    "print('rank_at_10_A_shap: ', np.mean(rank_at_10_A_shap[1:])/a)\n",
    "print('rank_at_20_A_shap: ', np.mean(rank_at_20_A_shap[1:])/a)\n",
    "print('rank_at_50_A_shap: ', np.mean(rank_at_50_A_shap[1:])/a)\n",
    "print('rank_at_100_A_shap: ', np.mean(rank_at_100_A_shap[1:])/a)\n",
    "print('rank_at_k_B_shap: ', np.mean(rank_at_k_B_shap[1:])/a)\n",
    "print('NDCG_at_1_shap: ', np.mean(NDCG_at_1_shap[1:])/a)\n",
    "print('NDCG_at_5_shap: ', np.mean(NDCG_at_5_shap[1:])/a)\n",
    "print('NDCG_at_10_shap: ', np.mean(NDCG_at_10_shap[1:])/a)\n",
    "print('NDCG_at_20_shap: ', np.mean(NDCG_at_20_shap[1:])/a)\n",
    "print('NDCG_at_50_shap: ', np.mean(NDCG_at_50_shap[1:])/a)\n",
    "print('NDCG_at_100_shap: ', np.mean(NDCG_at_100_shap[1:])/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3d5bf-e771-4a62-9bde-2f44520cd9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55589580-03a1-4bf6-86db-4745f08291e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['POS_at_1_j_g', 'POS_at_5_j_g', 'POS_at_10_j_g', 'POS_at_20_j_g', 'POS_at_50_j_g', 'POS_at_100_j_g', 'NEG_at_1_j_g', 'NEG_at_5_j_g', 'NEG_at_10_j_g', 'NEG_at_20_j_g', 'NEG_at_50_j_g', 'NEG_at_100_j_g', 'users_DEL_j_g', 'users_INS_j_g', 'rank_at_1_j_g', 'rank_at_5_j_g', 'rank_at_10_A_j_g', 'rank_at_20_A_j_g', 'rank_at_50_A_j_g', 'rank_at_100_A_j_g', 'rank_at_k_B_j_g', 'NDCG_at_1_j_g', 'NDCG_at_5_j_g', 'NDCG_at_10_j_g', 'NDCG_at_20_j_g', 'NDCG_at_50_j_g', 'NDCG_at_100_j_g','POS_at_1_c_s', 'POS_at_5_c_s', 'POS_at_10_c_s', 'POS_at_20_c_s', 'POS_at_50_c_s', 'POS_at_100_c_s', 'NEG_at_1_c_s', 'NEG_at_5_c_s', 'NEG_at_10_c_s', 'NEG_at_20_c_s', 'NEG_at_50_c_s', 'NEG_at_100_c_s', 'users_DEL_c_s', 'users_INS_c_s', 'rank_at_1_c_s', 'rank_at_5_c_s', 'rank_at_10_A_c_s', 'rank_at_20_A_c_s', 'rank_at_50_A_c_s', 'rank_at_100_A_c_s', 'rank_at_k_B_c_s', 'NDCG_at_1_c_s', 'NDCG_at_5_c_s', 'NDCG_at_10_c_s', 'NDCG_at_20_c_s', 'NDCG_at_50_c_s', 'NDCG_at_100_c_s,\n",
    "'POS_at_1_lime', 'POS_at_5_lime', 'POS_at_10_lime', 'POS_at_20_lime', 'POS_at_50_lime', 'POS_at_100_lime', 'NEG_at_1_lime', 'NEG_at_5_lime', 'NEG_at_10_lime', 'NEG_at_20_lime', 'NEG_at_50_lime', 'NEG_at_100_lime', 'users_DEL_lime', 'users_INS_lime', 'rank_at_1_lime', 'rank_at_5_lime', 'rank_at_10_A_lime', 'rank_at_20_A_lime', 'rank_at_50_A_lime', 'rank_at_100_A_lime', 'rank_at_k_B_lime', 'NDCG_at_1_lime', 'NDCG_at_5_lime', 'NDCG_at_10_lime', 'NDCG_at_20_lime', 'NDCG_at_50_lime', 'NDCG_at_100_lime',\n",
    "'POS_at_1_ltx', 'POS_at_5_ltx', 'POS_at_10_ltx', 'POS_at_20_ltx', 'POS_at_50_ltx', 'POS_at_100_ltx', 'NEG_at_1_ltx', 'NEG_at_5_ltx', 'NEG_at_10_ltx', 'NEG_at_20_ltx', 'NEG_at_50_ltx', 'NEG_at_100_ltx', 'users_DEL_ltx', 'users_INS_ltx', 'rank_at_1_ltx', 'rank_at_5_ltx', 'rank_at_10_A_ltx', 'rank_at_20_A_ltx', 'rank_at_50_A_ltx', 'rank_at_100_A_ltx', 'rank_at_k_B_ltx', 'NDCG_at_1_ltx', 'NDCG_at_5_ltx', 'NDCG_at_10_ltx', 'NDCG_at_20_ltx', 'NDCG_at_50_ltx', 'NDCG_at_100_ltx', 'POS_at_1_tf_idf', 'POS_at_5_tf_idf', 'POS_at_10_tf_idf', 'POS_at_20_tf_idf', 'POS_at_50_tf_idf', 'POS_at_100_tf_idf', 'NEG_at_1_tf_idf', 'NEG_at_5_tf_idf', 'NEG_at_10_tf_idf', 'NEG_at_20_tf_idf', 'NEG_at_50_tf_idf', 'NEG_at_100_tf_idf', 'users_DEL_tf_idf', 'users_INS_tf_idf', 'rank_at_1_tf_idf', 'rank_at_5_tf_idf', 'rank_at_10_A_tf_idf', 'rank_at_20_A_tf_idf', 'rank_at_50_A_tf_idf', 'rank_at_100_A_tf_idf', 'rank_at_k_B_tf_idf', 'NDCG_at_1_tf_idf', 'NDCG_at_5_tf_idf', 'NDCG_at_10_tf_idf', 'NDCG_at_20_tf_idf', 'NDCG_at_50_tf_idf', 'NDCG_at_100_tf_idf,POS_at_1_pop', 'POS_at_5_pop', 'POS_at_10_pop', 'POS_at_20_pop', 'POS_at_50_pop', 'POS_at_100_pop', 'NEG_at_1_pop', 'NEG_at_5_pop', 'NEG_at_10_pop', 'NEG_at_20_pop', 'NEG_at_50_pop', 'NEG_at_100_pop', 'users_DEL_pop', 'users_INS_pop', 'rank_at_1_pop', 'rank_at_5_pop', 'rank_at_10_A_pop', 'rank_at_20_A_pop', 'rank_at_50_A_pop', 'rank_at_100_A_pop', 'rank_at_k_B_pop', 'NDCG_at_1_pop', 'NDCG_at_5_pop', 'NDCG_at_10_pop', 'NDCG_at_20_pop', 'NDCG_at_50_pop', 'NDCG_at_100_pop]\n",
    "\n",
    "values = [POS_at_1_j_g, pos_at_5_j_g, POS_at_10_j_g, POS_at_20_j_g, POS_at_50_j_g, POS_at_100_j_g, NEG_at_1_j_g, NEG_at_5_j_g, NEG_at_10_j_g, NEG_at_20_j_g, NEG_at_50_j_g, NEG_at_100_j_g, users_DEL_j_g, users_INS_j_g, rank_at_1_j_g, rank_at_5_j_g, rank_at_10_A_j_g, rank_at_20_A_j_g, rank_at_50_A_j_g, rank_at_100_A_j_g, rank_at_k_B_j_g, NDCG_at_1_j_g, NDCG_at_5_j_g, NDCG_at_10_j_g, NDCG_at_20_j_g, NDCG_at_50_j_g, NDCG_at_100_j_g, POS_at_1_c_s, pos_at_5_c_s, POS_at_10_c_s, POS_at_20_c_s, POS_at_50_c_s, POS_at_100_c_s, NEG_at_1_c_s, NEG_at_5_c_s, NEG_at_10_c_s, NEG_at_20_c_s, NEG_at_50_c_s, NEG_at_100_c_s, users_DEL_c_s, users_INS_c_s, rank_at_1_c_s, rank_at_5_c_s, rank_at_10_A_c_s, rank_at_20_A_c_s, rank_at_50_A_c_s, rank_at_100_A_c_s, rank_at_k_B_c_s, NDCG_at_1_c_s, NDCG_at_5_c_s, NDCG_at_10_c_s, NDCG_at_20_c_s, NDCG_at_50_c_s, NDCG_at_100_c_s,POS_at_1_lime, pos_at_5_lime, POS_at_10_lime, POS_at_20_lime, POS_at_50_lime, POS_at_100_lime, NEG_at_1_lime, NEG_at_5_lime, NEG_at_10_lime, NEG_at_20_lime, NEG_at_50_lime, NEG_at_100_lime, users_DEL_lime, users_INS_lime, rank_at_1_lime, rank_at_5_lime, rank_at_10_A_lime, rank_at_20_A_lime, rank_at_50_A_lime, rank_at_100_A_lime, rank_at_k_B_lime, NDCG_at_1_lime, NDCG_at_5_lime, NDCG_at_10_lime, NDCG_at_20_lime, NDCG_at_50_lime, NDCG_at_100_lime, POS_at_1_ltx, pos_at_5_ltx, POS_at_10_ltx, POS_at_20_ltx, POS_at_50_ltx, POS_at_100_ltx, NEG_at_1_ltx, NEG_at_5_ltx, NEG_at_10_ltx, NEG_at_20_ltx, NEG_at_50_ltx, NEG_at_100_ltx, users_DEL_ltx, users_INS_ltx, rank_at_1_ltx, rank_at_5_ltx, rank_at_10_A_ltx, rank_at_20_A_ltx, rank_at_50_A_ltx, rank_at_100_A_ltx, rank_at_k_B_ltx, NDCG_at_1_ltx, NDCG_at_5_ltx, NDCG_at_10_ltx, NDCG_at_20_ltx, NDCG_at_50_ltx, NDCG_at_100_ltx, POS_at_1_tf_idf, pos_at_5_tf_idf, POS_at_10_tf_idf, POS_at_20_tf_idf, POS_at_50_tf_idf, POS_at_100_tf_idf, NEG_at_1_tf_idf, NEG_at_5_tf_idf, NEG_at_10_tf_idf, NEG_at_20_tf_idf, NEG_at_50_tf_idf, NEG_at_100_tf_idf, users_DEL_tf_idf, users_INS_tf_idf, rank_at_1_tf_idf, rank_at_5_tf_idf, rank_at_10_A_tf_idf, rank_at_20_A_tf_idf, rank_at_50_A_tf_idf, rank_at_100_A_tf_idf, rank_at_k_B_tf_idf, NDCG_at_1_tf_idf, NDCG_at_5_tf_idf, NDCG_at_10_tf_idf, NDCG_at_20_tf_idf, NDCG_at_50_tf_idf, NDCG_at_100_tf_idf, POS_at_1_pop, pos_at_5_pop, POS_at_10_pop, POS_at_20_pop, POS_at_50_pop, POS_at_100_pop, NEG_at_1_pop, NEG_at_5_pop, NEG_at_10_pop, NEG_at_20_pop, NEG_at_50_pop, NEG_at_100_pop, users_DEL_pop, users_INS_pop, rank_at_1_pop, rank_at_5_pop, rank_at_10_A_pop, rank_at_20_A_pop, rank_at_50_A_pop, rank_at_100_A_pop, rank_at_k_B_pop, NDCG_at_1_pop, NDCG_at_5_pop, NDCG_at_10_pop, NDCG_at_20_pop, NDCG_at_50_pop, NDCG_at_100_pop]\n",
    "\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "MLP_results_ml1m = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "MLP_results_ml1m['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = MLP_results_ml1m['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "MLP_results_ml1m['AUC'] = AUC\n",
    "\n",
    "MLP_results_ml1m.head()\n",
    "\n",
    "MLP_results_ml1m.to_csv('MLP_19_4_results_ml1m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f437f-e6da-410c-b07a-93a0eaa542e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f50421-c721-462f-8ec0-ac7768943e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67ef58-2dbc-4d98-92e4-13d3fe6ac9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
