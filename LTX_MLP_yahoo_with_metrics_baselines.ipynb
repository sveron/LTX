{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cb33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "from scipy import sparse\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c524e08d-8212-499c-8517-9c0f287ff729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47a2c4eb-91a5-4852-b8d4-a8e23311e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7222810-6617-42cb-9794-b3b194fd981e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'items_values_dict_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(items_values_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    items_values_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209689b-ec97-495f-b6f1-ca3e1ab04d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d69ab8-fcc7-4985-9ffc-c4edc9404fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'user_similarities_Jaccard_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(user_similarities_Jaccard, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    user_similarities_Jaccard = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ada5339-cb46-407d-851b-f208738eb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'cosine_items_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(cosine_items_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    cosine_items_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3e4efa-1c12-4045-8eb4-43fb9047dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = cosine_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e027c20-6f3d-4bfb-96ea-64ded575f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'tf_idf_items_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "#with open(file_path, 'wb') as f:\n",
    "    #pickle.dump(cosine_items_dict, f)\n",
    "with open(file_path, 'rb') as f:\n",
    "    tf_idf_items = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450a0368-41f8-4501-9e32-059966c3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'pop_dict_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    popularity_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29de5b25-1cdd-498f-9f32-ac9602cbf7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5ac75b-5892-4227-9a30-7048e3c43b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ipynb\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "'''\n",
    "distance_to_proximity(distances_list) - takes distances from origin user and returns proximity\n",
    "LimeBase() - class that gets kernel function\n",
    "get_lime_args(user_vetor, item_id, model, items_array, min_pert = 10, max_pert = 20, num_of_perturbations = 5, seed = 0) - \n",
    "    returns neighborhood_data, neighborhood_labels, distances, item_id \n",
    "'''\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1666c-8f99-4cf9-bfb7-85417eeaa0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479c204f-a9cb-4d97-bccd-a536e4192411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5e7530-31df-4896-a82f-ca5cbf513e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.linear_y = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_representation = self.linear_x(user.float())\n",
    "        item_representation = self.linear_y(item.float())\n",
    "        dot_prod = torch.matmul(user_representation, item_representation.T)\n",
    "        dot_sigmoid = self.sigmoid(dot_prod)\n",
    "        \n",
    "        return dot_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041f1d25-56b4-413f-9676-f249d709d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender_G(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size):\n",
    "        super(Recommender_G, self).__init__()\n",
    "        self.mlp = MLP_G(num_items, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, user_vector, item_vector):\n",
    "        user_vector = user_vector.to(device)\n",
    "        item_vector = item_vector.to(device)\n",
    "        output = self.mlp(user_vector, item_vector)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ffa3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Explainer_G(nn.Module):\n",
    "    def __init__(self, recommender_model_g, input_size, hidden_size):\n",
    "        super(Explainer_G, self).__init__()\n",
    "        \n",
    "        backbone_children = list(recommender_model_g.children())[0]\n",
    "\n",
    "        self.slice1 = nn.Sequential(*list(backbone_children.children())[:1])\n",
    "        self.slice2 = nn.Sequential(*list(backbone_children.children())[1:2])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=input_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        slice1_output = self.slice1(user.float())\n",
    "        slice2_output = self.slice2(item.float())\n",
    "        combined_output = torch.cat((slice1_output, slice2_output), dim=-1)\n",
    "        mask = self.bottleneck(combined_output).to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb926432-c8ef-4b77-8666-e914ba3ad651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "class LossModelCombined(torch.nn.Module):\n",
    "    def __init__(self,alpha_parameter, recommender_model_g, explainer_model_g, hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "            \n",
    "        self.recommender_model_g =  recommender_model_g\n",
    "        self.explainer_model_g= explainer_model_g\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_parameter = alpha_parameter\n",
    "\n",
    "        \n",
    "    def forward(self,user_hist, y_true):\n",
    "        \n",
    "        user_hist = user_hist.to(device)\n",
    "        y_true = torch.tensor(y_true).float().to(device)\n",
    "        \n",
    "        mask = self.explainer_model_g(user_hist,y_true).to(device)\n",
    "         # \"weakened\" history \n",
    "        x_masked = user_hist * mask\n",
    "        y_item_id = np.argmax(y_true.cpu().detach().numpy())\n",
    "        \n",
    "        y_masked = self.recommender_model_g(x_masked,y_true).unsqueeze(0).to(device)      \n",
    "        #cross entropy between the new \"masked\" prediction and the original one\n",
    "        cross_entropy_loss = -torch.log(y_masked).to(device)\n",
    "        #l1 loss on mask\n",
    "        l1_loss = torch.mean(torch.abs(mask)).to(device)\n",
    "        #bce_loss = torch.nn.BCELoss(mask[y_item_id], y_true[y_item_id])#-torch.log(1-mask[y_item_id]).to(device)\n",
    "        #combined loss\n",
    "        comb_loss = cross_entropy_loss + self.alpha_parameter * l1_loss\n",
    "        \n",
    "        \n",
    "        #print(\"Cross-entropy loss: {:.4f}, L1 loss: {:.4f}\".format(cross_entropy_loss.item(), bce_loss.item()))\n",
    "        \n",
    "        return x_masked, comb_loss  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee527d7-301d-4f84-a2a7-3729f4af99bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.nn.functional as F\\nclass LossModelCombined(torch.nn.Module):\\n    def __init__(self,alpha_parameter, beta_parameter, recommender_model, explainer_model, hidden_size):\\n     \\n        super().__init__()\\n            \\n        self.recommender_model =  recommender_model\\n        self.explainer_model= explainer_model\\n        self.hidden_size = hidden_size\\n        self.alpha_parameter = alpha_parameter\\n        self.beta_parameter = beta_parameter\\n        \\n    def forward(self,user_hist, y_positive, y_negative=None):\\n      \\n        user_hist = user_hist.to(device)\\n        y_positive = torch.tensor(y_positive).float().to(device)\\n        \\n        mask = self.explainer_model(user_hist,y_positive).to(device)\\n        x_masked = user_hist * mask\\n        y_positive_masked = self.recommender_model(x_masked,y_positive).unsqueeze(0).to(device)      \\n        pred_loss = -torch.log(y_positive_masked).to(device)\\n        \\n        pred_negative_loss = 0\\n        if y_negative is not None:\\n            y_negative_masked = self.recommender_model(x_masked,y_negative).unsqueeze(0).to(device) \\n            pred_negative_loss = torch.log(y_negative_masked).to(device)\\n\\n        \\n        mask_loss = torch.mean(torch.abs(mask)).to(device)\\n   \\n        comb_loss = pred_loss + self.alpha_parameter * mask_loss + self.beta_parameter*pred_negative_loss \\n        \\n        #print(\"Pred loss: {:.4f}, L1 loss: {:.4f}, Negative Loss {:.4f}\".format(pred_loss.item(), mask_loss.item(), pred_negative_loss.item()))\\n        \\n        return x_masked, comb_loss \\n        '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be59aebc-807c-41d0-8ea3-bfec4683898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train and test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96e4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users is  13725\n",
      "num_items is  10265\n"
     ]
    }
   ],
   "source": [
    "# Train the model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "hidden_dim = 20\n",
    "num_users = 13725\n",
    "num_items = 10265\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37cab73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model = Recommender_G(num_items, hidden_dim)\n",
    "rec_model.load_state_dict(torch.load('recommender_model_yahoo.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fd3b70a-bb23-4a27-a5a6-6f45e604e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in rec_model.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19ed0c5-c088-4c13-ba61-2d59b4cda1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59a95439-e2de-4bfb-aec4-46d7274b7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed = pd.read_csv('train_data_mixed_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f451f90-03f1-4207-ab58-ff1ee7ebd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb80d99e-0525-4a09-b4c7-6be306a86bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10258</th>\n",
       "      <th>10259</th>\n",
       "      <th>10260</th>\n",
       "      <th>10261</th>\n",
       "      <th>10262</th>\n",
       "      <th>10263</th>\n",
       "      <th>10264</th>\n",
       "      <th>user_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>y_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7987</td>\n",
       "      <td>1</td>\n",
       "      <td>5303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7987</td>\n",
       "      <td>0</td>\n",
       "      <td>3309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10191</td>\n",
       "      <td>1</td>\n",
       "      <td>7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10191</td>\n",
       "      <td>0</td>\n",
       "      <td>2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3867</td>\n",
       "      <td>1</td>\n",
       "      <td>7389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  10258  10259  10260  10261  10262  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "\n",
       "   10263  10264  user_id  interaction  y_values  \n",
       "0      0      0     7987            1      5303  \n",
       "1      0      0     7987            0      3309  \n",
       "2      0      0    10191            1      7053  \n",
       "3      0      0    10191            0      2273  \n",
       "4      0      0     3867            1      7389  \n",
       "\n",
       "[5 rows x 10268 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_mixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d198ccb-e89f-4350-b9e9-4f786148bd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10257</th>\n",
       "      <th>10258</th>\n",
       "      <th>10259</th>\n",
       "      <th>10260</th>\n",
       "      <th>10261</th>\n",
       "      <th>10262</th>\n",
       "      <th>10263</th>\n",
       "      <th>10264</th>\n",
       "      <th>user_id</th>\n",
       "      <th>y_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4891</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12745</td>\n",
       "      <td>7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9355</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6963</td>\n",
       "      <td>6269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10601</td>\n",
       "      <td>6542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  10257  10258  10259  10260  10261  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "\n",
       "   10262  10263  10264  user_id  y_positive  \n",
       "0      0      0      0     4891        3025  \n",
       "1      0      0      0    12745        7426  \n",
       "2      0      0      0     9355        5693  \n",
       "3      0      0      0     6963        6269  \n",
       "4      0      0      0    10601        6542  \n",
       "\n",
       "[5 rows x 10267 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dad84345-6533-4680-9dd8-1b14137fd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('items_values_dict_Yahoo.pkl', 'rb') as f:\n",
    "    items_values_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5492be4d-77a2-4027-bf9c-02a59b4e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_values= pd.read_csv('items_values_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bfc5b9b-dd0b-4eca-9e86-3ad934735248",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6a10384-2767-45f2-9f38-149755c9b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "253a1f53-6f41-4fea-a2a2-ffb619d1d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top k dictionaries for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a154033f-5fed-41a0-88f7-26de7d2e664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_train_Yahoo.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c47f1600-5173-425b-b909-5ce178903677",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_test_Yahoo.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53ffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "524e4244-be51-407b-8526-31e761b4b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e1b2bed-e049-43af-972f-26c13e700e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k(user_vector, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    item_tensor = torch.FloatTensor(items_array).to(device)\n",
    "    output_model = [float(i) for i in model(user_tensor, item_tensor).cpu().detach().numpy()]\n",
    "    \n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        item_prob_dict[i]=output[i]\n",
    "\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf100df-077d-4140-bc76-695ea4ce8fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_in_the_list(user_vector, original_user_vector, item_id, num_items, model):\n",
    "    top_k_list = list(get_top_k(user_vector, original_user_vector, num_items, model, num_items).keys())\n",
    "    return top_k_list.index(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195a9425-d6fa-4b50-948c-6bb7ff2e96ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5addf221-c6e9-49d1-9c2f-c0776c2aff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ndcg(ranked_list, target_item):\n",
    "    if target_item not in ranked_list:\n",
    "        return 0.0\n",
    "\n",
    "    target_idx = torch.tensor(ranked_list.index(target_item), device=device)\n",
    "    ndcg = torch.reciprocal(torch.log2(target_idx + 2))\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "988248e6-ac2d-405a-94ec-bd7ca2761841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, recommender_model, model_combined, mask_type = None):\n",
    "    \n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "    \n",
    "    item_vector = items_values_dict[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    \n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(np.sum(user_vector)), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    rankA_at_1 = [0]*(len(bins))\n",
    "    rankA_at_5 = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    rankB = [0]*(len(bins))\n",
    "    \n",
    "    NDCG_at_1 = [0]*(len(bins))\n",
    "    NDCG_at_5 = [0]*(len(bins))\n",
    "    NDCG_at_10 = [0]*(len(bins))\n",
    "    NDCG_at_20 = [0]*(len(bins))\n",
    "    NDCG_at_50 = [0]*(len(bins))\n",
    "    NDCG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    total_items = 0\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            if mask_type == 'jaccard_g':\n",
    "                sim_items = find_jaccard_g_mask(POS_masked, item_id, num_items, jaccard_based_sim)\n",
    "            elif mask_type == 'jaccard_u':\n",
    "                sim_items = find_jaccard_u_mask(POS_masked, item_id, num_items, user_similarities_Jaccard)\n",
    "            elif mask_type == 'cosine':\n",
    "                sim_items = find_cosine_mask(POS_masked, item_id, num_items, cosine_items)\n",
    "            elif mask_type == 'pop':\n",
    "                sim_items = find_pop_mask(POS_masked, item_id, num_items)\n",
    "            elif mask_type == 'tf_idf':\n",
    "                sim_items = find_tf_idf_mask(POS_masked, item_id, num_items, tf_idf_items)\n",
    "            elif mask_type == 'lime':\n",
    "                sim_items = find_LIME_mask(user_vector, item_id, items_array, 50, 100, 50, distance_to_proximity,'highest_weights',recommender_model, num_samples=user_hist_size)\n",
    "            elif mask_type == 'ltx':\n",
    "                sim_items = find_LTX_mask(POS_masked, item_vector, item_id, model_combined)   \n",
    "            else:\n",
    "                raise Exception(\"Wrong mask type\")\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))[0:user_hist_size]\n",
    "        \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked\n",
    "        \n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "\n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor,num_items, recommender_model,num_items)        \n",
    "        POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        NEG_index = get_index_in_the_list(NEG_masked,user_tensor, item_id, num_items, recommender_model)+1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_model(POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_model(user_tensor-POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "        # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1+1-POS_index)/10)\n",
    "        rankA_at_5[i] = max(0, (5+1-POS_index)/20)\n",
    "        rankA_at_10[i] = max(0, (10+1-POS_index)/10)\n",
    "        rankA_at_20[i] = max(0, (20+1-POS_index)/20)\n",
    "        rankA_at_50[i] = max(0, (50+1-POS_index)/50)\n",
    "        rankA_at_100[i] = max(0, (100+1-POS_index)/100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1/POS_index\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG_at_1[i]= get_ndcg(list(POS_ranked_list.keys())[:1],item_id)\n",
    "        NDCG_at_5[i]= get_ndcg(list(POS_ranked_list.keys())[:5],item_id)\n",
    "        NDCG_at_10[i]= get_ndcg(list(POS_ranked_list.keys())[:10],item_id)\n",
    "        NDCG_at_20[i]= get_ndcg(list(POS_ranked_list.keys())[:20],item_id)\n",
    "        NDCG_at_50[i]= get_ndcg(list(POS_ranked_list.keys())[:50],item_id)\n",
    "        NDCG_at_100[i]= get_ndcg(list(POS_ranked_list.keys())[:100],item_id)\n",
    "\n",
    "    res = [POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100, DEL, INS, rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100, rankB, NDCG_at_1, NDCG_at_5, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19286437-e93f-4a10-8201-dbb16e5363d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6def03ae-370e-4b49-845d-c8cd367eb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, recommender_model, model_combined, mask_type = None):\n",
    "    \n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "    \n",
    "    item_vector = items_values_dict[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    \n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(np.sum(user_vector)), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    rankB = [0]*(len(bins))\n",
    "    NDCG_at_10 = [0]*(len(bins))\n",
    "    NDCG_at_20 = [0]*(len(bins))\n",
    "    NDCG_at_50 = [0]*(len(bins))\n",
    "    NDCG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    total_items = 0\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            if mask_type == 'jaccard_g':\n",
    "                sim_items = find_jaccard_g_mask(POS_masked, item_id, num_items, jaccard_based_sim)\n",
    "            elif mask_type == 'jaccard_u':\n",
    "                sim_items = find_jaccard_u_mask(POS_masked, item_id, num_items, user_similarities_Jaccard)\n",
    "            elif mask_type == 'cosine':\n",
    "                sim_items = find_cosine_mask(POS_masked, item_id, num_items, cosine_items)\n",
    "            elif mask_type == 'tf_idf':\n",
    "                sim_items = find_tf_idf_mask(POS_masked, item_id, num_items, tf_idf_items)\n",
    "            elif mask_type == 'pop':\n",
    "                sim_items = find_pop_mask(POS_masked, item_id, num_items)\n",
    "            elif mask_type == 'lime':\n",
    "                sim_items = find_LIME_mask(user_vector, item_id, items_array, 50, 100, 400, distance_to_proximity,'highest_weights',recommender_model, num_samples=user_hist_size)\n",
    "            elif mask_type == 'ltx':\n",
    "                sim_items = find_LTX_mask(POS_masked, item_vector, item_id, model_combined)   \n",
    "            else:\n",
    "                raise Exception(\"Wrong mask type\")\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "        \n",
    "        \n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "\n",
    "\n",
    "        NEG_index = get_index_in_the_list(NEG_masked,user_tensor, item_id, num_items, recommender_model)+1\n",
    "   \n",
    "        \n",
    "        # for neg:\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "        \n",
    " \n",
    "            \n",
    "    res = [POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100, DEL, INS, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100, rankB, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "    return res  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7796d852-24e2-45a1-af3f-87290c718cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d42338-5589-4575-9d2e-9f19d91b004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LIME_mask(x, item_id, items_array, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, model, num_samples=10, method = 'POS'):\n",
    "    \n",
    "    user_hist = x \n",
    "    # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = mlp_get_lime_args(user_hist, item_id, model, items_array, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id)\n",
    "                                                                                  \n",
    "    most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    most_pop_items_dict =  {key: value for key, value in most_pop_items}\n",
    "    \n",
    "    return most_pop_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5f8b5d6-617e-4ff1-bdac-7ad826053bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX based similarity\n",
    "def find_LTX_mask(x, y_true, item_id, model_combined):\n",
    "    \n",
    "    user_hist = torch.tensor(x) \n",
    "    user_hist[item_id] = 0 \n",
    "    \n",
    "    x_masked_g, loss_comb_g = model_combined(user_hist, y_true)\n",
    "    \n",
    "    x_masked_g = x_masked_g.to(device)\n",
    "    #item_sim_dict = {i: v for i, v in enumerate(x_masked_l)}\n",
    "    item_sim_dict = {i: x_masked_g[i].item() for i in range(x_masked_g.numel())}\n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "617c4c4a-de4c-42e5-8572-3acf8ad93355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Genre based similarities using Jaccard\n",
    "def find_jaccard_g_mask(x, item_id, num_items,jaccard_based_sim):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in jaccard_based_sim.keys()):\n",
    "                item_jaccard_dict[i]=jaccard_based_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "    \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01a902eb-5acc-40b9-a170-c7025aee3b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_u_mask(x, item_id, num_items, user_based_Jaccard_sim):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in user_based_Jaccard_sim.keys()):\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "            \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ad6767b-0c7b-49de-8a9e-625ddb771891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, num_items, item_cosine):\n",
    "\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "        \n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in item_cosine.keys()):\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)] # add cosine similarity between items\n",
    "            else:\n",
    "                item_cosine_dict[i] = 0\n",
    "    \n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cc8fa68-dc09-4707-88a3-07eda457c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf mask\n",
    "def find_tf_idf_mask(x, item_id, num_items, tf_idf_sim):\n",
    "    \n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "    \n",
    "    positive_items = np.where(x == 1)[0]\n",
    "    tf_idf_dict = {i: tf_idf_sim.get((i, item_id), 0) for i in positive_items}\n",
    "    \n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "960f4701-a38b-485d-bc34-942d4fe5e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#popularity mask\n",
    "def find_pop_mask(x, item_id, num_items):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate over all positive items of the user\n",
    "            item_pop_dict[i]=popularity_dict[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8f86e27-a3f6-46ff-a2fa-7e3742e827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "075a4a8b-04b6-4391-a70a-bce22d900344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get users vectors to create topk\n",
    "unique_indices = np.unique(train_array[:,-3], return_index=True, axis=0)[1]\n",
    "\n",
    "# create a new array with only the unique users\n",
    "train_unique_arr = train_array[unique_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe054c3b-9c29-4f77-82f5-06ffba6dc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_model_g = Explainer_G(rec_model, num_items, hidden_dim).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb999b3f-c39c-4cb1-b2e5-32387d242246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.0467\n",
      "Epoch 1, Train Loss 0.0149\n",
      "Epoch 2, Train Loss 0.0087\n",
      "Epoch 3, Train Loss 0.0063\n",
      "Epoch 4, Train Loss 0.0058\n",
      "Epoch 5, Train Loss 0.0055\n",
      "Epoch 6, Train Loss 0.0047\n",
      "Epoch 7, Train Loss 0.0035\n",
      "Epoch 8, Train Loss 0.0038\n",
      "Epoch 9, Train Loss 0.0030\n",
      "Epoch 10, Train Loss 0.0034\n",
      "Epoch 11, Train Loss 0.0031\n",
      "Epoch 12, Train Loss 0.0031\n"
     ]
    }
   ],
   "source": [
    "#LTX training\n",
    "\n",
    "import time\n",
    "import random\n",
    "train_losses = []\n",
    "epochs = 13\n",
    "#torch.manual_seed(42)\n",
    "hidden_dim = 20\n",
    "num_of_bins = 10\n",
    "alpha_parameter = 0.1\n",
    "#beta_parameter = 0.1\n",
    "\n",
    "model_combined = LossModelCombined(alpha_parameter,rec_model, explainer_model_g, hidden_dim).to(device)\n",
    "optimizer_comb = torch.optim.Adam(model_combined.parameters(), lr=0.01)\n",
    "    \n",
    "#Train LTX\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(train_unique_arr.shape[0]):\n",
    "        \n",
    "        #user data\n",
    "        user_id = train_unique_arr[i][-3]\n",
    "        user_vector = train_unique_arr[i][:-3]\n",
    "        #get top1 of this user for LTX training\n",
    "        \n",
    "        top1_item = np.argmax(topk_train[user_id])\n",
    "    \n",
    "        #top_indices = np.argsort(list(topk_train[user_id]))[-100:-1]\n",
    "        # Sample one index randomly\n",
    "        #top_100_indices = np.delete(top_indices, np.where(top_indices == top1_item))\n",
    "    \n",
    "        # sample an index from the top 100\n",
    "        #not_top1 = np.random.choice(top_100_indices)\n",
    "        #print(not_top1)\n",
    "        #print(\"not_top1_item \", not_top1)\n",
    "       \n",
    "        positive_sample = torch.FloatTensor(items_values_dict[top1_item]).to(device)\n",
    "        #negative_sample = torch.FloatTensor(items_values_dict[not_top1]).to(device)\n",
    "        \n",
    "        user_vector[top1_item] = 0 \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        #train model        \n",
    "        optimizer_comb.zero_grad()\n",
    "        x_masked_g, loss_comb_g = model_combined(user_tensor, positive_sample) #, negative_sample)\n",
    "        \n",
    "        train_loss+=loss_comb_g.item()\n",
    "            \n",
    "        loss_comb_g.backward()\n",
    "        optimizer_comb.step()\n",
    "        \n",
    "  \n",
    "    train_losses.append(train_loss/train_unique_arr.shape[0])\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss {train_loss/train_unique_arr.shape[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6490426-daf5-401f-8bde-f767660afb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model_combined.state_dict(), 'model_combined_yahoo_13_01.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19ab9629-7c3a-4f52-9779-4940bd3d7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(explainer_model_g.state_dict(), 'explainer_model_yahoo_13_01.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7459b47-e7da-4d2a-ba08-bd7f10c2d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTX on testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f437f-e6da-410c-b07a-93a0eaa542e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Evaluate the model on the test set\n",
    "num_of_bins = 11\n",
    "k = 100\n",
    "\n",
    "\n",
    "\n",
    "POS_at_1_j_g = np.zeros(num_of_bins)\n",
    "pos_at_5_j_g = np.zeros(num_of_bins)\n",
    "POS_at_10_j_g = np.zeros(num_of_bins)\n",
    "POS_at_20_j_g = np.zeros(num_of_bins)\n",
    "POS_at_50_j_g = np.zeros(num_of_bins)\n",
    "POS_at_100_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_g = np.zeros(num_of_bins)\n",
    "users_DEL_j_g = np.zeros(num_of_bins)\n",
    "users_INS_j_g = np.zeros(num_of_bins)\n",
    "rank_at_1_j_g = np.zeros(num_of_bins)\n",
    "rank_at_5_j_g = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_g = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_j_u = np.zeros(num_of_bins)\n",
    "pos_at_5_j_u = np.zeros(num_of_bins)\n",
    "POS_at_10_j_u = np.zeros(num_of_bins)\n",
    "POS_at_20_j_u = np.zeros(num_of_bins)\n",
    "POS_at_50_j_u = np.zeros(num_of_bins)\n",
    "POS_at_100_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_u = np.zeros(num_of_bins)\n",
    "users_DEL_j_u = np.zeros(num_of_bins)\n",
    "users_INS_j_u = np.zeros(num_of_bins)\n",
    "rank_at_1_j_u = np.zeros(num_of_bins)\n",
    "rank_at_5_j_u = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_u = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_c_s = np.zeros(num_of_bins)\n",
    "pos_at_5_c_s = np.zeros(num_of_bins)\n",
    "POS_at_10_c_s = np.zeros(num_of_bins)\n",
    "POS_at_20_c_s = np.zeros(num_of_bins)\n",
    "POS_at_50_c_s = np.zeros(num_of_bins)\n",
    "POS_at_100_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_20_c_s= np.zeros(num_of_bins)\n",
    "NEG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_100_c_s = np.zeros(num_of_bins)\n",
    "users_DEL_c_s = np.zeros(num_of_bins)\n",
    "users_INS_c_s = np.zeros(num_of_bins)\n",
    "rank_at_1_c_s = np.zeros(num_of_bins)\n",
    "rank_at_5_c_s = np.zeros(num_of_bins)\n",
    "rank_at_10_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_20_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_50_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_100_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_k_B_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_20_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_100_c_s = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_pop = np.zeros(num_of_bins)\n",
    "pos_at_5_pop = np.zeros(num_of_bins)\n",
    "POS_at_10_pop = np.zeros(num_of_bins)\n",
    "POS_at_20_pop = np.zeros(num_of_bins)\n",
    "POS_at_50_pop = np.zeros(num_of_bins)\n",
    "POS_at_100_pop = np.zeros(num_of_bins)\n",
    "NEG_at_1_pop = np.zeros(num_of_bins)\n",
    "NEG_at_5_pop = np.zeros(num_of_bins)\n",
    "NEG_at_10_pop = np.zeros(num_of_bins)\n",
    "NEG_at_20_pop = np.zeros(num_of_bins)\n",
    "NEG_at_50_pop = np.zeros(num_of_bins)\n",
    "NEG_at_100_pop = np.zeros(num_of_bins)\n",
    "users_DEL_pop = np.zeros(num_of_bins)\n",
    "users_INS_pop = np.zeros(num_of_bins)\n",
    "rank_at_1_pop = np.zeros(num_of_bins)\n",
    "rank_at_5_pop = np.zeros(num_of_bins)\n",
    "rank_at_10_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_20_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_50_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_100_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_k_B_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_1_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_5_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_10_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_20_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_50_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_100_pop = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lime = np.zeros(num_of_bins)\n",
    "pos_at_5_lime = np.zeros(num_of_bins)\n",
    "POS_at_10_lime = np.zeros(num_of_bins)\n",
    "POS_at_20_lime = np.zeros(num_of_bins)\n",
    "POS_at_50_lime = np.zeros(num_of_bins)\n",
    "POS_at_100_lime = np.zeros(num_of_bins)\n",
    "NEG_at_1_lime = np.zeros(num_of_bins)\n",
    "NEG_at_5_lime = np.zeros(num_of_bins)\n",
    "NEG_at_10_lime = np.zeros(num_of_bins)\n",
    "NEG_at_20_lime = np.zeros(num_of_bins)\n",
    "NEG_at_50_lime = np.zeros(num_of_bins)\n",
    "NEG_at_100_lime = np.zeros(num_of_bins)\n",
    "users_DEL_lime = np.zeros(num_of_bins)\n",
    "users_INS_lime = np.zeros(num_of_bins)\n",
    "rank_at_1_lime = np.zeros(num_of_bins)\n",
    "rank_at_5_lime = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lime = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "pos_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "users_DEL_tf_idf = np.zeros(num_of_bins)\n",
    "users_INS_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_10_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_20_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_50_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_100_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_k_B_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_ltx = np.zeros(num_of_bins)\n",
    "pos_at_5_ltx = np.zeros(num_of_bins)\n",
    "POS_at_10_ltx = np.zeros(num_of_bins)\n",
    "POS_at_20_ltx = np.zeros(num_of_bins)\n",
    "POS_at_50_ltx = np.zeros(num_of_bins)\n",
    "POS_at_100_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_1_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_5_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_100_ltx = np.zeros(num_of_bins)\n",
    "users_DEL_ltx = np.zeros(num_of_bins)\n",
    "users_INS_ltx = np.zeros(num_of_bins)\n",
    "rank_at_1_ltx = np.zeros(num_of_bins)\n",
    "rank_at_5_ltx = np.zeros(num_of_bins)\n",
    "rank_at_10_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_20_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_50_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_100_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_k_B_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_1_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_5_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_100_ltx = np.zeros(num_of_bins)\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "explainer_model_g.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        #item_id = test_array[i][-1]\n",
    "        user_id = test_array[i][-2]\n",
    "\n",
    "        #item_vector = items_values.iloc[item_id]\n",
    "        user_vector = test_array[i][:-2]\n",
    "\n",
    "        y_pred = topk_test[user_id]\n",
    "        item_id = np.argmax(y_pred)\n",
    "\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        ### Jaccard user:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'jaccard_u')\n",
    "        POS_at_1_j_u += res[0]\n",
    "        pos_at_5_j_u += res[1]\n",
    "        POS_at_10_j_u += res[2]\n",
    "        POS_at_20_j_u += res[3]\n",
    "        POS_at_50_j_u += res[4]\n",
    "        POS_at_100_j_u += res[5]\n",
    "        NEG_at_1_j_u += res[6]\n",
    "        NEG_at_5_j_u += res[7]\n",
    "        NEG_at_10_j_u += res[8]\n",
    "        NEG_at_20_j_u += res[9]\n",
    "        NEG_at_50_j_u += res[10]\n",
    "        NEG_at_100_j_u += res[11]\n",
    "        users_DEL_j_u += res[12]\n",
    "        users_INS_j_u += res[13]\n",
    "        rank_at_1_j_u += res[14]\n",
    "        rank_at_5_j_u += res[15]\n",
    "        rank_at_10_A_j_u += res[16]\n",
    "        rank_at_20_A_j_u += res[17]\n",
    "        rank_at_50_A_j_u += res[18]\n",
    "        rank_at_100_A_j_u += res[19]\n",
    "        rank_at_k_B_j_u += res[20]\n",
    "        NDCG_at_1_j_u += res[21]\n",
    "        NDCG_at_5_j_u += res[22]\n",
    "        NDCG_at_10_j_u += res[23]\n",
    "        NDCG_at_20_j_u += res[24]\n",
    "        NDCG_at_50_j_u += res[25]\n",
    "        NDCG_at_100_j_u += res[26]\n",
    "\n",
    "        ### cosine similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'cosine')\n",
    "        POS_at_1_c_s += res[0]\n",
    "        pos_at_5_c_s += res[1]\n",
    "        POS_at_10_c_s += res[2]\n",
    "        POS_at_20_c_s += res[3]\n",
    "        POS_at_50_c_s += res[4]\n",
    "        POS_at_100_c_s += res[5]\n",
    "        NEG_at_1_c_s += res[6]\n",
    "        NEG_at_5_c_s += res[7]\n",
    "        NEG_at_10_c_s += res[8]\n",
    "        NEG_at_20_c_s += res[9]\n",
    "        NEG_at_50_c_s += res[10]\n",
    "        NEG_at_100_c_s += res[11]\n",
    "        users_DEL_c_s += res[12]\n",
    "        users_INS_c_s += res[13]\n",
    "        rank_at_1_c_s += res[14]\n",
    "        rank_at_5_c_s += res[15]\n",
    "        rank_at_10_A_c_s += res[16]\n",
    "        rank_at_20_A_c_s += res[17]\n",
    "        rank_at_50_A_c_s += res[18]\n",
    "        rank_at_100_A_c_s += res[19]\n",
    "        rank_at_k_B_c_s += res[20]\n",
    "        NDCG_at_1_c_s += res[21]\n",
    "        NDCG_at_5_c_s += res[22]\n",
    "        NDCG_at_10_c_s += res[23]\n",
    "        NDCG_at_20_c_s += res[24]\n",
    "        NDCG_at_50_c_s += res[25]\n",
    "        NDCG_at_100_c_s += res[26]\n",
    "        \n",
    "        ## tf-idf similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'tf_idf')\n",
    "        POS_at_1_tf_idf += res[0]\n",
    "        pos_at_5_tf_idf += res[1]\n",
    "        POS_at_10_tf_idf += res[2]\n",
    "        POS_at_20_tf_idf += res[3]\n",
    "        POS_at_50_tf_idf += res[4]\n",
    "        POS_at_100_tf_idf += res[5]\n",
    "        NEG_at_1_tf_idf += res[6]\n",
    "        NEG_at_5_tf_idf += res[7]\n",
    "        NEG_at_10_tf_idf += res[8]\n",
    "        NEG_at_20_tf_idf += res[9]\n",
    "        NEG_at_50_tf_idf += res[10]\n",
    "        NEG_at_100_tf_idf += res[11]\n",
    "        users_DEL_tf_idf += res[12]\n",
    "        users_INS_tf_idf += res[13]\n",
    "        rank_at_1_tf_idf += res[14]\n",
    "        rank_at_5_tf_idf += res[15]\n",
    "        rank_at_10_A_tf_idf += res[16]\n",
    "        rank_at_20_A_tf_idf += res[17]\n",
    "        rank_at_50_A_tf_idf += res[18]\n",
    "        rank_at_100_A_tf_idf += res[19]\n",
    "        rank_at_k_B_tf_idf += res[20]\n",
    "        NDCG_at_1_tf_idf += res[21]\n",
    "        NDCG_at_5_tf_idf += res[22]\n",
    "        NDCG_at_10_tf_idf += res[23]\n",
    "        NDCG_at_20_tf_idf += res[24]\n",
    "        NDCG_at_50_tf_idf += res[25]\n",
    "        NDCG_at_100_tf_idf += res[26]\n",
    "\n",
    "        ### pop:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'pop')    \n",
    "        POS_at_1_pop += res[0]\n",
    "        pos_at_5_pop += res[1]\n",
    "        POS_at_10_pop += res[2]\n",
    "        POS_at_20_pop += res[3]\n",
    "        POS_at_50_pop += res[4]\n",
    "        POS_at_100_pop += res[5]\n",
    "        NEG_at_1_pop += res[6]\n",
    "        NEG_at_5_pop += res[7]\n",
    "        NEG_at_10_pop += res[8]\n",
    "        NEG_at_20_pop += res[9]\n",
    "        NEG_at_50_pop += res[10]\n",
    "        NEG_at_100_pop += res[11]\n",
    "        users_DEL_pop += res[12]\n",
    "        users_INS_pop += res[13]\n",
    "        rank_at_1_pop += res[14]\n",
    "        rank_at_5_pop += res[15]\n",
    "        rank_at_10_A_pop += res[16]\n",
    "        rank_at_20_A_pop += res[17]\n",
    "        rank_at_50_A_pop += res[18]\n",
    "        rank_at_100_A_pop += res[19]\n",
    "        rank_at_k_B_pop += res[20]\n",
    "        NDCG_at_1_pop += res[21]\n",
    "        NDCG_at_5_pop += res[22]\n",
    "        NDCG_at_10_pop += res[23]\n",
    "        NDCG_at_20_pop += res[24]\n",
    "        NDCG_at_50_pop += res[25]\n",
    "        NDCG_at_100_pop += res[26]\n",
    "\n",
    "        ### LTX:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'ltx')    \n",
    "        POS_at_1_ltx += res[0]\n",
    "        pos_at_5_ltx += res[1]\n",
    "        POS_at_10_ltx += res[2]\n",
    "        POS_at_20_ltx += res[3]\n",
    "        POS_at_50_ltx += res[4]\n",
    "        POS_at_100_ltx += res[5]\n",
    "        NEG_at_1_ltx += res[6]\n",
    "        NEG_at_5_ltx += res[7]\n",
    "        NEG_at_10_ltx += res[8]\n",
    "        NEG_at_20_ltx += res[9]\n",
    "        NEG_at_50_ltx += res[10]\n",
    "        NEG_at_100_ltx += res[11]\n",
    "        users_DEL_ltx += res[12]\n",
    "        users_INS_ltx += res[13]\n",
    "        rank_at_1_ltx += res[14]\n",
    "        rank_at_5_ltx += res[15]\n",
    "        rank_at_10_A_ltx += res[16]\n",
    "        rank_at_20_A_ltx += res[17]\n",
    "        rank_at_50_A_ltx += res[18]\n",
    "        rank_at_100_A_ltx += res[19]\n",
    "        rank_at_k_B_ltx += res[20]\n",
    "        NDCG_at_1_ltx += res[21]\n",
    "        NDCG_at_5_ltx += res[22]\n",
    "        NDCG_at_10_ltx += res[23]\n",
    "        NDCG_at_20_ltx += res[24]\n",
    "        NDCG_at_50_ltx += res[25]\n",
    "        NDCG_at_100_ltx += res[26]\n",
    "        \n",
    "        ### Lime:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lime')    \n",
    "        POS_at_1_lime += res[0]\n",
    "        pos_at_5_lime += res[1]\n",
    "        POS_at_10_lime += res[2]\n",
    "        POS_at_20_lime += res[3]\n",
    "        POS_at_50_lime += res[4]\n",
    "        POS_at_100_lime += res[5]\n",
    "        NEG_at_1_lime += res[6]\n",
    "        NEG_at_5_lime += res[7]\n",
    "        NEG_at_10_lime += res[8]\n",
    "        NEG_at_20_lime += res[9]\n",
    "        NEG_at_50_lime += res[10]\n",
    "        NEG_at_100_lime += res[11]\n",
    "        users_DEL_lime += res[12]\n",
    "        users_INS_lime += res[13]\n",
    "        rank_at_1_lime += res[14]\n",
    "        rank_at_5_lime += res[15]\n",
    "        rank_at_10_A_lime += res[16]\n",
    "        rank_at_20_A_lime += res[17]\n",
    "        rank_at_50_A_lime += res[18]\n",
    "        rank_at_100_A_lime += res[19]\n",
    "        rank_at_k_B_lime += res[20]\n",
    "        NDCG_at_1_lime += res[21]\n",
    "        NDCG_at_5_lime += res[22]\n",
    "        NDCG_at_10_lime += res[23]\n",
    "        NDCG_at_20_lime += res[24]\n",
    "        NDCG_at_50_lime += res[25]\n",
    "        NDCG_at_100_lime += res[26]\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "           \n",
    "a = i+1\n",
    "\n",
    "print('POS_at_1_j_u: ', np.mean(POS_at_1_j_u[1:])/a)\n",
    "print('pos_at_5_j_u: ', np.mean(pos_at_5_j_u[1:])/a)\n",
    "print('POS_at_10_j_u: ', np.mean(POS_at_10_j_u[1:])/a)\n",
    "print('POS_at_20_j_u: ', np.mean(POS_at_20_j_u[1:])/a)\n",
    "print('POS_at_50_j_u: ', np.mean(POS_at_50_j_u[1:])/a)\n",
    "print('POS_at_100_j_u: ', np.mean(POS_at_100_j_u[1:])/a)\n",
    "print('NEG_at_1_j_u: ', np.mean(NEG_at_1_j_u[1:])/a)\n",
    "print('NEG_at_5_j_u: ', np.mean(NEG_at_5_j_u[1:])/a)\n",
    "print('NEG_at_10_j_u: ', np.mean(NEG_at_10_j_u[1:])/a)\n",
    "print('NEG_at_20_j_u: ', np.mean(NEG_at_20_j_u[1:])/a)\n",
    "print('NEG_at_50_j_u: ', np.mean(NEG_at_50_j_u[1:])/a)\n",
    "print('NEG_at_100_j_u: ', np.mean(NEG_at_100_j_u[1:])/a)\n",
    "print('users_DEL_j_u: ', np.mean(users_DEL_j_u[1:])/a)\n",
    "print('users_INS_j_u: ', np.mean(users_INS_j_u[1:])/a)\n",
    "print('rank_at_1_j_u: ', np.mean(rank_at_1_j_u[1:])/a)\n",
    "print('rank_at_5_j_u: ', np.mean(rank_at_5_j_u[1:])/a)\n",
    "print('rank_at_10_A_j_u: ', np.mean(rank_at_10_A_j_u[1:])/a)\n",
    "print('rank_at_20_A_j_u: ', np.mean(rank_at_20_A_j_u[1:])/a)\n",
    "print('rank_at_50_A_j_u: ', np.mean(rank_at_50_A_j_u[1:])/a)\n",
    "print('rank_at_100_A_j_u: ', np.mean(rank_at_100_A_j_u[1:])/a)\n",
    "print('rank_at_k_B_j_u: ', np.mean(rank_at_k_B_j_u[1:])/a)\n",
    "print('NDCG_at_1_j_u: ', np.mean(NDCG_at_1_j_u[1:])/a)\n",
    "print('NDCG_at_5_j_u: ', np.mean(NDCG_at_5_j_u[1:])/a)\n",
    "print('NDCG_at_10_j_u: ', np.mean(NDCG_at_10_j_u[1:])/a)\n",
    "print('NDCG_at_20_j_u: ', np.mean(NDCG_at_20_j_u[1:])/a)\n",
    "print('NDCG_at_50_j_u: ', np.mean(NDCG_at_50_j_u[1:])/a)\n",
    "print('NDCG_at_100_j_u: ', np.mean(NDCG_at_100_j_u[1:])/a)\n",
    "\n",
    "print('POS_at_1_j_g: ', np.mean(POS_at_1_j_g[1:])/a)\n",
    "print('pos_at_5_j_g: ', np.mean(pos_at_5_j_g[1:])/a)\n",
    "print('POS_at_10_j_g: ', np.mean(POS_at_10_j_g[1:])/a)\n",
    "print('POS_at_20_j_g: ', np.mean(POS_at_20_j_g[1:])/a)\n",
    "print('POS_at_50_j_g: ', np.mean(POS_at_50_j_g[1:])/a)\n",
    "print('POS_at_100_j_g: ', np.mean(POS_at_100_j_g[1:])/a)\n",
    "print('NEG_at_1_j_g: ', np.mean(NEG_at_1_j_g[1:])/a)\n",
    "print('NEG_at_5_j_g: ', np.mean(NEG_at_5_j_g[1:])/a)\n",
    "print('NEG_at_10_j_g: ', np.mean(NEG_at_10_j_g[1:])/a)\n",
    "print('NEG_at_20_j_g: ', np.mean(NEG_at_20_j_g[1:])/a)\n",
    "print('NEG_at_50_j_g: ', np.mean(NEG_at_50_j_g[1:])/a)\n",
    "print('NEG_at_100_j_g: ', np.mean(NEG_at_100_j_g[1:])/a)\n",
    "print('users_DEL_j_g: ', np.mean(users_DEL_j_g[1:])/a)\n",
    "print('users_INS_j_g: ', np.mean(users_INS_j_g[1:])/a)\n",
    "print('rank_at_1_j_g: ', np.mean(rank_at_1_j_g[1:])/a)\n",
    "print('rank_at_5_j_g: ', np.mean(rank_at_5_j_g[1:])/a)\n",
    "print('rank_at_10_A_j_g: ', np.mean(rank_at_10_A_j_g[1:])/a)\n",
    "print('rank_at_20_A_j_g: ', np.mean(rank_at_20_A_j_g[1:])/a)\n",
    "print('rank_at_50_A_j_g: ', np.mean(rank_at_50_A_j_g[1:])/a)\n",
    "print('rank_at_100_A_j_g: ', np.mean(rank_at_100_A_j_g[1:])/a)\n",
    "print('rank_at_k_B_j_g: ', np.mean(rank_at_k_B_j_g[1:])/a)\n",
    "print('NDCG_at_1_j_g: ', np.mean(NDCG_at_1_j_g[1:])/a)\n",
    "print('NDCG_at_5_j_g: ', np.mean(NDCG_at_5_j_g[1:])/a)\n",
    "print('NDCG_at_10_j_g: ', np.mean(NDCG_at_10_j_g[1:])/a)\n",
    "print('NDCG_at_20_j_g: ', np.mean(NDCG_at_20_j_g[1:])/a)\n",
    "print('NDCG_at_50_j_g: ', np.mean(NDCG_at_50_j_g[1:])/a)\n",
    "print('NDCG_at_100_j_g: ', np.mean(NDCG_at_100_j_g[1:])/a)\n",
    "\n",
    "print('POS_at_1_c_s: ', np.mean(POS_at_1_c_s[1:])/a)\n",
    "print('pos_at_5_c_s: ', np.mean(pos_at_5_c_s[1:])/a)\n",
    "print('POS_at_10_c_s: ', np.mean(POS_at_10_c_s[1:])/a)\n",
    "print('POS_at_20_c_s: ', np.mean(POS_at_20_c_s[1:])/a)\n",
    "print('POS_at_50_c_s: ', np.mean(POS_at_50_c_s[1:])/a)\n",
    "print('POS_at_100_c_s: ', np.mean(POS_at_100_c_s[1:])/a)\n",
    "print('NEG_at_1_c_s: ', np.mean(NEG_at_1_c_s[1:])/a)\n",
    "print('NEG_at_5_c_s: ', np.mean(NEG_at_5_c_s[1:])/a)\n",
    "print('NEG_at_10_c_s: ', np.mean(NEG_at_10_c_s[1:])/a)\n",
    "print('NEG_at_20_c_s: ', np.mean(NEG_at_20_c_s[1:])/a)\n",
    "print('NEG_at_50_c_s: ', np.mean(NEG_at_50_c_s[1:])/a)\n",
    "print('NEG_at_100_c_s: ', np.mean(NEG_at_100_c_s[1:])/a)\n",
    "print('users_DEL_c_s: ', np.mean(users_DEL_c_s[1:])/a)\n",
    "print('users_INS_c_s: ', np.mean(users_INS_c_s[1:])/a)\n",
    "print('rank_at_1_c_s: ', np.mean(rank_at_1_c_s[1:])/a)\n",
    "print('rank_at_5_c_s: ', np.mean(rank_at_5_c_s[1:])/a)\n",
    "print('rank_at_10_A_c_s: ', np.mean(rank_at_10_A_c_s[1:])/a)\n",
    "print('rank_at_20_A_c_s: ', np.mean(rank_at_20_A_c_s[1:])/a)\n",
    "print('rank_at_50_A_c_s: ', np.mean(rank_at_50_A_c_s[1:])/a)\n",
    "print('rank_at_100_A_c_s: ', np.mean(rank_at_100_A_c_s[1:])/a)\n",
    "print('rank_at_k_B_c_s: ', np.mean(rank_at_k_B_c_s[1:])/a)\n",
    "print('NDCG_at_1_c_s: ', np.mean(NDCG_at_1_c_s[1:])/a)\n",
    "print('NDCG_at_5_c_s: ', np.mean(NDCG_at_5_c_s[1:])/a)\n",
    "print('NDCG_at_10_c_s: ', np.mean(NDCG_at_10_c_s[1:])/a)\n",
    "print('NDCG_at_20_c_s: ', np.mean(NDCG_at_20_c_s[1:])/a)\n",
    "print('NDCG_at_50_c_s: ', np.mean(NDCG_at_50_c_s[1:])/a)\n",
    "print('NDCG_at_100_c_s: ', np.mean(NDCG_at_100_c_s[1:])/a)\n",
    "\n",
    "print('POS_at_1_pop: ', np.mean(POS_at_1_pop[1:])/a)\n",
    "print('pos_at_5_pop: ', np.mean(pos_at_5_pop[1:])/a)\n",
    "print('POS_at_10_pop: ', np.mean(POS_at_10_pop[1:])/a)\n",
    "print('POS_at_20_pop: ', np.mean(POS_at_20_pop[1:])/a)\n",
    "print('POS_at_50_pop: ', np.mean(POS_at_50_pop[1:])/a)\n",
    "print('POS_at_100_pop: ', np.mean(POS_at_100_pop[1:])/a)\n",
    "print('NEG_at_1_pop: ', np.mean(NEG_at_1_pop[1:])/a)\n",
    "print('NEG_at_5_pop: ', np.mean(NEG_at_5_pop[1:])/a)\n",
    "print('NEG_at_10_pop: ', np.mean(NEG_at_10_pop[1:])/a)\n",
    "print('NEG_at_20_pop: ', np.mean(NEG_at_20_pop[1:])/a)\n",
    "print('NEG_at_50_pop: ', np.mean(NEG_at_50_pop[1:])/a)\n",
    "print('NEG_at_100_pop: ', np.mean(NEG_at_100_pop[1:])/a)\n",
    "print('users_DEL_pop: ', np.mean(users_DEL_pop[1:])/a)\n",
    "print('users_INS_pop: ', np.mean(users_INS_pop[1:])/a)\n",
    "print('rank_at_1_pop: ', np.mean(rank_at_1_pop[1:])/a)\n",
    "print('rank_at_5_pop: ', np.mean(rank_at_5_pop[1:])/a)\n",
    "print('rank_at_10_A_pop: ', np.mean(rank_at_10_A_pop[1:])/a)\n",
    "print('rank_at_20_A_pop: ', np.mean(rank_at_20_A_pop[1:])/a)\n",
    "print('rank_at_50_A_pop: ', np.mean(rank_at_50_A_pop[1:])/a)\n",
    "print('rank_at_100_A_pop: ', np.mean(rank_at_100_A_pop[1:])/a)\n",
    "print('rank_at_k_B_pop: ', np.mean(rank_at_k_B_pop[1:])/a)\n",
    "print('NDCG_at_1_pop: ', np.mean(NDCG_at_1_pop[1:])/a)\n",
    "print('NDCG_at_5_pop: ', np.mean(NDCG_at_5_pop[1:])/a)\n",
    "print('NDCG_at_10_pop: ', np.mean(NDCG_at_10_pop[1:])/a)\n",
    "print('NDCG_at_20_pop: ', np.mean(NDCG_at_20_pop[1:])/a)\n",
    "print('NDCG_at_50_pop: ', np.mean(NDCG_at_50_pop[1:])/a)\n",
    "print('NDCG_at_100_pop: ', np.mean(NDCG_at_100_pop[1:])/a)\n",
    "\n",
    "print('POS_at_1_lime: ', np.mean(POS_at_1_lime[1:])/a)\n",
    "print('pos_at_5_lime: ', np.mean(pos_at_5_lime[1:])/a)\n",
    "print('POS_at_10_lime: ', np.mean(POS_at_10_lime[1:])/a)\n",
    "print('POS_at_20_lime: ', np.mean(POS_at_20_lime[1:])/a)\n",
    "print('POS_at_50_lime: ', np.mean(POS_at_50_lime[1:])/a)\n",
    "print('POS_at_100_lime: ', np.mean(POS_at_100_lime[1:])/a)\n",
    "print('NEG_at_1_lime: ', np.mean(NEG_at_1_lime[1:])/a)\n",
    "print('NEG_at_5_lime: ', np.mean(NEG_at_5_lime[1:])/a)\n",
    "print('NEG_at_10_lime: ', np.mean(NEG_at_10_lime[1:])/a)\n",
    "print('NEG_at_20_lime: ', np.mean(NEG_at_20_lime[1:])/a)\n",
    "print('NEG_at_50_lime: ', np.mean(NEG_at_50_lime[1:])/a)\n",
    "print('NEG_at_100_lime: ', np.mean(NEG_at_100_lime[1:])/a)\n",
    "print('users_DEL_lime: ', np.mean(users_DEL_lime[1:])/a)\n",
    "print('users_INS_lime: ', np.mean(users_INS_lime[1:])/a)\n",
    "print('rank_at_1_lime: ', np.mean(rank_at_1_lime[1:])/a)\n",
    "print('rank_at_5_lime: ', np.mean(rank_at_5_lime[1:])/a)\n",
    "print('rank_at_10_A_lime: ', np.mean(rank_at_10_A_lime[1:])/a)\n",
    "print('rank_at_20_A_lime: ', np.mean(rank_at_20_A_lime[1:])/a)\n",
    "print('rank_at_50_A_lime: ', np.mean(rank_at_50_A_lime[1:])/a)\n",
    "print('rank_at_100_A_lime: ', np.mean(rank_at_100_A_lime[1:])/a)\n",
    "print('rank_at_k_B_lime: ', np.mean(rank_at_k_B_lime[1:])/a)\n",
    "print('NDCG_at_1_lime: ', np.mean(NDCG_at_1_lime[1:])/a)\n",
    "print('NDCG_at_5_lime: ', np.mean(NDCG_at_5_lime[1:])/a)\n",
    "print('NDCG_at_10_lime: ', np.mean(NDCG_at_10_lime[1:])/a)\n",
    "print('NDCG_at_20_lime: ', np.mean(NDCG_at_20_lime[1:])/a)\n",
    "print('NDCG_at_50_lime: ', np.mean(NDCG_at_50_lime[1:])/a)\n",
    "print('NDCG_at_100_lime: ', np.mean(NDCG_at_100_lime[1:])/a)\n",
    "\n",
    "print('POS_at_1_tf_idf: ', np.mean(POS_at_1_tf_idf[1:])/a)\n",
    "print('pos_at_5_tf_idf: ', np.mean(pos_at_5_tf_idf[1:])/a)\n",
    "print('POS_at_10_tf_idf: ', np.mean(POS_at_10_tf_idf[1:])/a)\n",
    "print('POS_at_20_tf_idf: ', np.mean(POS_at_20_tf_idf[1:])/a)\n",
    "print('POS_at_50_tf_idf: ', np.mean(POS_at_50_tf_idf[1:])/a)\n",
    "print('POS_at_100_tf_idf: ', np.mean(POS_at_100_tf_idf[1:])/a)\n",
    "print('NEG_at_1_tf_idf: ', np.mean(NEG_at_1_tf_idf[1:])/a)\n",
    "print('NEG_at_5_tf_idf: ', np.mean(NEG_at_5_tf_idf[1:])/a)\n",
    "print('NEG_at_10_tf_idf: ', np.mean(NEG_at_10_tf_idf[1:])/a)\n",
    "print('NEG_at_20_tf_idf: ', np.mean(NEG_at_20_tf_idf[1:])/a)\n",
    "print('NEG_at_50_tf_idf: ', np.mean(NEG_at_50_tf_idf[1:])/a)\n",
    "print('NEG_at_100_tf_idf: ', np.mean(NEG_at_100_tf_idf[1:])/a)\n",
    "print('users_DEL_tf_idf: ', np.mean(users_DEL_tf_idf[1:])/a)\n",
    "print('users_INS_tf_idf: ', np.mean(users_INS_tf_idf[1:])/a)\n",
    "print('rank_at_1_tf_idf: ', np.mean(rank_at_1_tf_idf[1:])/a)\n",
    "print('rank_at_5_tf_idf: ', np.mean(rank_at_5_tf_idf[1:])/a)\n",
    "print('rank_at_10_A_tf_idf: ', np.mean(rank_at_10_A_tf_idf[1:])/a)\n",
    "print('rank_at_20_A_tf_idf: ', np.mean(rank_at_20_A_tf_idf[1:])/a)\n",
    "print('rank_at_50_A_tf_idf: ', np.mean(rank_at_50_A_tf_idf[1:])/a)\n",
    "print('rank_at_100_A_tf_idf: ', np.mean(rank_at_100_A_tf_idf[1:])/a)\n",
    "print('rank_at_k_B_tf_idf: ', np.mean(rank_at_k_B_tf_idf[1:])/a)\n",
    "print('NDCG_at_1_tf_idf: ', np.mean(NDCG_at_1_tf_idf[1:])/a)\n",
    "print('NDCG_at_5_tf_idf: ', np.mean(NDCG_at_5_tf_idf[1:])/a)\n",
    "print('NDCG_at_10_tf_idf: ', np.mean(NDCG_at_10_tf_idf[1:])/a)\n",
    "print('NDCG_at_20_tf_idf: ', np.mean(NDCG_at_20_tf_idf[1:])/a)\n",
    "print('NDCG_at_50_tf_idf: ', np.mean(NDCG_at_50_tf_idf[1:])/a)\n",
    "print('NDCG_at_100_tf_idf: ', np.mean(NDCG_at_100_tf_idf[1:])/a)\n",
    "\n",
    "print('POS_at_1_ltx: ', np.mean(POS_at_1_ltx[1:])/a)\n",
    "print('pos_at_5_ltx: ', np.mean(pos_at_5_ltx[1:])/a)\n",
    "print('POS_at_10_ltx: ', np.mean(POS_at_10_ltx[1:])/a)\n",
    "print('POS_at_20_ltx: ', np.mean(POS_at_20_ltx[1:])/a)\n",
    "print('POS_at_50_ltx: ', np.mean(POS_at_50_ltx[1:])/a)\n",
    "print('POS_at_100_ltx: ', np.mean(POS_at_100_ltx[1:])/a)\n",
    "print('NEG_at_1_ltx: ', np.mean(NEG_at_1_ltx[1:])/a)\n",
    "print('NEG_at_5_ltx: ', np.mean(NEG_at_5_ltx[1:])/a)\n",
    "print('NEG_at_10_ltx: ', np.mean(NEG_at_10_ltx[1:])/a)\n",
    "print('NEG_at_20_ltx: ', np.mean(NEG_at_20_ltx[1:])/a)\n",
    "print('NEG_at_50_ltx: ', np.mean(NEG_at_50_ltx[1:])/a)\n",
    "print('NEG_at_100_ltx: ', np.mean(NEG_at_100_ltx[1:])/a)\n",
    "print('users_DEL_ltx: ', np.mean(users_DEL_ltx[1:])/a)\n",
    "print('users_INS_ltx: ', np.mean(users_INS_ltx[1:])/a)\n",
    "print('rank_at_1_ltx: ', np.mean(rank_at_1_ltx[1:])/a)\n",
    "print('rank_at_5_ltx: ', np.mean(rank_at_5_ltx[1:])/a)\n",
    "print('rank_at_10_A_ltx: ', np.mean(rank_at_10_A_ltx[1:])/a)\n",
    "print('rank_at_20_A_ltx: ', np.mean(rank_at_20_A_ltx[1:])/a)\n",
    "print('rank_at_50_A_ltx: ', np.mean(rank_at_50_A_ltx[1:])/a)\n",
    "print('rank_at_100_A_ltx: ', np.mean(rank_at_100_A_ltx[1:])/a)\n",
    "print('rank_at_k_B_ltx: ', np.mean(rank_at_k_B_ltx[1:])/a)\n",
    "print('NDCG_at_1_ltx: ', np.mean(NDCG_at_1_ltx[1:])/a)\n",
    "print('NDCG_at_5_ltx: ', np.mean(NDCG_at_5_ltx[1:])/a)\n",
    "print('NDCG_at_10_ltx: ', np.mean(NDCG_at_10_ltx[1:])/a)\n",
    "print('NDCG_at_20_ltx: ', np.mean(NDCG_at_20_ltx[1:])/a)\n",
    "print('NDCG_at_50_ltx: ', np.mean(NDCG_at_50_ltx[1:])/a)\n",
    "print('NDCG_at_100_ltx: ', np.mean(NDCG_at_100_ltx[1:])/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "451de05a-bc8d-4ac1-9b07-58843e540ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "POS_at_10_lime 0.17894353369763205\n",
      "POS_at_20_lime 0.225063752276867\n",
      "POS_at_50_lime 0.3081967213114754\n",
      "POS_at_100_lime 0.3943169398907104\n",
      "NEG_at_10_lime 0.5574134790528232\n",
      "NEG_at_20_lime 0.6199635701275046\n",
      "NEG_at_50_lime 0.7037158469945355\n",
      "NEG_at_100_lime 0.771912568306011\n",
      "users_DEL_lime 0.7144514225977017\n",
      "users_INS_lime 0.9868090570800286\n",
      "rank_at_10_A_lime 0.1411001821493624\n",
      "rank_at_20_A_lime 0.172874316939891\n",
      "rank_at_50_A_lime 0.23223096539162155\n",
      "rank_at_100_A_lime 0.2944571948998188\n",
      "rank_at_k_B_lime 0.11915433067784759\n",
      "NDCG_at_10_lime 0.12694719490132045\n",
      "NDCG_at_20_lime 0.13848198213972462\n",
      "NDCG_at_50_lime 0.1548899496117359\n",
      "NDCG_at_100_lime 0.16885818025670632\n",
      "POS_at_10_ltx 0.20127504553734063\n",
      "POS_at_20_ltx 0.25249544626593806\n",
      "POS_at_50_ltx 0.35267759562841533\n",
      "POS_at_100_ltx 0.447103825136612\n",
      "NEG_at_10_ltx 0.5377777777777778\n",
      "NEG_at_20_ltx 0.5966120218579235\n",
      "NEG_at_50_ltx 0.67959927140255\n",
      "NEG_at_100_ltx 0.7404735883424408\n",
      "users_DEL_ltx 0.7859043172869766\n",
      "users_INS_ltx 0.9861352397516792\n",
      "rank_at_10_A_ltx 0.16324590163934408\n",
      "rank_at_20_A_ltx 0.19714571948998189\n",
      "rank_at_50_A_ltx 0.2646185792349732\n",
      "rank_at_100_A_ltx 0.3351307832422598\n",
      "rank_at_k_B_ltx 0.14082394931412512\n",
      "NDCG_at_10_ltx 0.14792200022056456\n",
      "NDCG_at_20_ltx 0.1608022375488976\n",
      "NDCG_at_50_ltx 0.18061783257023667\n",
      "NDCG_at_100_ltx 0.1959249257427964\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "# Evaluate the model on the test set\n",
    "num_of_bins = 11\n",
    "k = 100\n",
    "\n",
    "POS_at_10_ltx = np.zeros(num_of_bins)\n",
    "POS_at_20_ltx = np.zeros(num_of_bins)\n",
    "POS_at_50_ltx = np.zeros(num_of_bins)\n",
    "POS_at_100_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NEG_at_100_ltx = np.zeros(num_of_bins)\n",
    "users_DEL_ltx = np.zeros(num_of_bins)\n",
    "users_INS_ltx = np.zeros(num_of_bins)\n",
    "rank_at_10_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_20_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_50_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_100_A_ltx = np.zeros(num_of_bins)\n",
    "rank_at_k_B_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_10_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_20_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_50_ltx = np.zeros(num_of_bins)\n",
    "NDCG_at_100_ltx = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_10_lime= np.zeros(num_of_bins)\n",
    "POS_at_20_lime = np.zeros(num_of_bins)\n",
    "POS_at_50_lime = np.zeros(num_of_bins)\n",
    "POS_at_100_lime = np.zeros(num_of_bins)\n",
    "NEG_at_10_lime = np.zeros(num_of_bins)\n",
    "NEG_at_20_lime = np.zeros(num_of_bins)\n",
    "NEG_at_50_lime = np.zeros(num_of_bins)\n",
    "NEG_at_100_lime = np.zeros(num_of_bins)\n",
    "users_DEL_lime = np.zeros(num_of_bins)\n",
    "users_INS_lime = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lime = np.zeros(num_of_bins)\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "explainer_model_g.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "\n",
    "        if i%500 == 0:\n",
    "            print(i)\n",
    "        #item_id = test_array[i][-1]\n",
    "        user_id = test_array[i][-2]\n",
    "\n",
    "        #item_vector = items_values.iloc[item_id]\n",
    "        user_vector = test_array[i][:-2]\n",
    "\n",
    "        y_pred = topk_test[user_id]\n",
    "        item_id = np.argmax(y_pred)\n",
    "\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        \n",
    "        ### Lime:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lime')    \n",
    "        POS_at_10_lime+= res[0]\n",
    "        POS_at_20_lime += res[1]\n",
    "        POS_at_50_lime += res[2]\n",
    "        POS_at_100_lime += res[3]\n",
    "        NEG_at_10_lime += res[4]\n",
    "        NEG_at_20_lime += res[5]\n",
    "        NEG_at_50_lime += res[6]\n",
    "        NEG_at_100_lime += res[7]\n",
    "        users_DEL_lime += res[8]\n",
    "        users_INS_lime += res[9]\n",
    "        rank_at_10_A_lime += res[10]\n",
    "        rank_at_20_A_lime += res[11]\n",
    "        rank_at_50_A_lime += res[12]\n",
    "        rank_at_100_A_lime += res[13]\n",
    "        rank_at_k_B_lime += res[14]\n",
    "        NDCG_at_10_lime += res[15]\n",
    "        NDCG_at_20_lime += res[16]\n",
    "        NDCG_at_50_lime += res[17]\n",
    "        NDCG_at_100_lime += res[18]\n",
    "        \n",
    "        ### LTX:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'ltx')    \n",
    "        POS_at_10_ltx += res[0]\n",
    "        POS_at_20_ltx += res[1]\n",
    "        POS_at_50_ltx += res[2]\n",
    "        POS_at_100_ltx += res[3]\n",
    "        NEG_at_10_ltx += res[4]\n",
    "        NEG_at_20_ltx += res[5]\n",
    "        NEG_at_50_ltx += res[6]\n",
    "        NEG_at_100_ltx += res[7]\n",
    "        users_DEL_ltx += res[8]\n",
    "        users_INS_ltx += res[9]\n",
    "        rank_at_10_A_ltx += res[10]\n",
    "        rank_at_20_A_ltx += res[11]\n",
    "        rank_at_50_A_ltx += res[12]\n",
    "        rank_at_100_A_ltx += res[13]\n",
    "        rank_at_k_B_ltx += res[14]\n",
    "        NDCG_at_10_ltx += res[15]\n",
    "        NDCG_at_20_ltx += res[16]\n",
    "        NDCG_at_50_ltx += res[17]\n",
    "        NDCG_at_100_ltx += res[18]\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "           \n",
    "a = i+1\n",
    "\n",
    "\n",
    "print(\"POS_at_10_lime\", np.mean(POS_at_10_lime[1:])/a)\n",
    "print(\"POS_at_20_lime\", np.mean(POS_at_20_lime[1:])/a)\n",
    "print(\"POS_at_50_lime\", np.mean(POS_at_50_lime[1:])/a)\n",
    "print(\"POS_at_100_lime\", np.mean(POS_at_100_lime[1:])/a)\n",
    "print(\"NEG_at_10_lime\", np.mean(NEG_at_10_lime[1:])/a)\n",
    "print(\"NEG_at_20_lime\", np.mean(NEG_at_20_lime[1:])/a)\n",
    "print(\"NEG_at_50_lime\", np.mean(NEG_at_50_lime[1:])/a)\n",
    "print(\"NEG_at_100_lime\", np.mean(NEG_at_100_lime[1:])/a)\n",
    "print(\"users_DEL_lime\", np.mean(users_DEL_lime[1:])/a)\n",
    "print(\"users_INS_lime\", np.mean(users_INS_lime[1:])/a)\n",
    "print(\"rank_at_10_A_lime\", np.mean(rank_at_10_A_lime[1:])/a)\n",
    "print(\"rank_at_20_A_lime\", np.mean(rank_at_20_A_lime[1:])/a)\n",
    "print(\"rank_at_50_A_lime\", np.mean(rank_at_50_A_lime[1:])/a)\n",
    "print(\"rank_at_100_A_lime\", np.mean(rank_at_100_A_lime[1:])/a)\n",
    "print(\"rank_at_k_B_lime\", np.mean(rank_at_k_B_lime[1:])/a)\n",
    "print('NDCG_at_10_lime', np.mean(NDCG_at_10_lime[1:])/a)\n",
    "print('NDCG_at_20_lime', np.mean(NDCG_at_20_lime[1:])/a)\n",
    "print('NDCG_at_50_lime', np.mean(NDCG_at_50_lime[1:])/a)\n",
    "print('NDCG_at_100_lime', np.mean(NDCG_at_100_lime[1:])/a)\n",
    "\n",
    "print(\"POS_at_10_ltx\", np.mean(POS_at_10_ltx[1:])/a)\n",
    "print(\"POS_at_20_ltx\", np.mean(POS_at_20_ltx[1:])/a)\n",
    "print(\"POS_at_50_ltx\", np.mean(POS_at_50_ltx[1:])/a)\n",
    "print(\"POS_at_100_ltx\", np.mean(POS_at_100_ltx[1:])/a)\n",
    "print(\"NEG_at_10_ltx\", np.mean(NEG_at_10_ltx[1:])/a)\n",
    "print(\"NEG_at_20_ltx\", np.mean(NEG_at_20_ltx[1:])/a)\n",
    "print(\"NEG_at_50_ltx\", np.mean(NEG_at_50_ltx[1:])/a)\n",
    "print(\"NEG_at_100_ltx\", np.mean(NEG_at_100_ltx[1:])/a)\n",
    "print(\"users_DEL_ltx\", np.mean(users_DEL_ltx[1:])/a)\n",
    "print(\"users_INS_ltx\", np.mean(users_INS_ltx[1:])/a)\n",
    "print(\"rank_at_10_A_ltx\", np.mean(rank_at_10_A_ltx[1:])/a)\n",
    "print(\"rank_at_20_A_ltx\", np.mean(rank_at_20_A_ltx[1:])/a)\n",
    "print(\"rank_at_50_A_ltx\", np.mean(rank_at_50_A_ltx[1:])/a)\n",
    "print(\"rank_at_100_A_ltx\", np.mean(rank_at_100_A_ltx[1:])/a)\n",
    "print(\"rank_at_k_B_ltx\", np.mean(rank_at_k_B_ltx[1:])/a)\n",
    "print('NDCG_at_10_ltx', np.mean(NDCG_at_10_ltx[1:])/a)\n",
    "print('NDCG_at_20_ltx', np.mean(NDCG_at_20_ltx[1:])/a)\n",
    "print('NDCG_at_50_ltx', np.mean(NDCG_at_50_ltx[1:])/a)\n",
    "print('NDCG_at_100_ltx', np.mean(NDCG_at_100_ltx[1:])/a)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61f50421-c721-462f-8ec0-ac7768943e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['POS_at_10_j_u', 'POS_at_20_j_u', 'POS_at_50_j_u', 'POS_at_100_j_u', 'NEG_at_10_j_u', 'NEG_at_20_j_u', 'NEG_at_50_j_u', 'NEG_at_100_j_u', 'users_DEL_j_u', 'users_INS_j_u', 'rank_at_10_A_j_u', 'rank_at_20_A_j_u', 'rank_at_50_A_j_u', 'rank_at_100_A_j_u', 'rank_at_k_B_j_u', 'NDCG_at_10_j_u', 'NDCG_at_20_j_u', 'NDCG_at_50_j_u', 'NDCG_at_100_j_u', 'POS_at_10_j_g', 'POS_at_20_j_g', 'POS_at_50_j_g', 'POS_at_100_j_g', 'NEG_at_10_j_g', 'NEG_at_20_j_g', 'NEG_at_50_j_g', 'NEG_at_100_j_g', 'users_DEL_j_g', 'users_INS_j_g', 'rank_at_10_A_j_g', 'rank_at_20_A_j_g', 'rank_at_50_A_j_g', 'rank_at_100_A_j_g', 'rank_at_k_B_j_g', 'NDCG_at_10_j_g', 'NDCG_at_20_j_g', 'NDCG_at_50_j_g', 'NDCG_at_100_j_g', 'POS_at_10_c_s', 'POS_at_20_c_s', 'POS_at_50_c_s', 'POS_at_100_c_s', 'NEG_at_10_c_s', 'NEG_at_20_c_s', 'NEG_at_50_c_s', 'NEG_at_100_c_s', 'users_DEL_c_s', 'users_INS_c_s', 'rank_at_10_A_c_s', 'rank_at_20_A_c_s', 'rank_at_50_A_c_s', 'rank_at_100_A_c_s', 'rank_at_k_B_c_s', 'NDCG_at_10_c_s', 'NDCG_at_20_c_s', 'NDCG_at_50_c_s', 'NDCG_at_100_c_s', 'POS_at_10_pop', 'POS_at_20_pop', 'POS_at_50_pop', 'POS_at_100_pop', 'NEG_at_10_pop', 'NEG_at_20_pop', 'NEG_at_50_pop', 'NEG_at_100_pop', 'users_DEL_pop', 'users_INS_pop', 'rank_at_10_A_pop', 'rank_at_20_A_pop', 'rank_at_50_A_pop', 'rank_at_100_A_pop', 'rank_at_k_B_pop', 'NDCG_at_10_pop', 'NDCG_at_20_pop', 'NDCG_at_50_pop', 'NDCG_at_100_pop', 'POS_at_10_lime', 'POS_at_20_lime', 'POS_at_50_lime', 'POS_at_100_lime', 'NEG_at_10_lime', 'NEG_at_20_lime', 'NEG_at_50_lime', 'NEG_at_100_lime', 'users_DEL_lime', 'users_INS_lime', 'rank_at_10_A_lime', 'rank_at_20_A_lime', 'rank_at_50_A_lime', 'rank_at_100_A_lime', 'rank_at_k_B_lime', 'NDCG_at_10_lime', 'NDCG_at_20_lime', 'NDCG_at_50_lime', 'NDCG_at_100_lime', 'POS_at_10_ltx', 'POS_at_20_ltx', 'POS_at_50_ltx', 'POS_at_100_ltx', 'NEG_at_10_ltx', 'NEG_at_20_ltx', 'NEG_at_50_ltx', 'NEG_at_100_ltx', 'users_DEL_ltx', 'users_INS_ltx', 'rank_at_10_A_ltx', 'rank_at_20_A_ltx', 'rank_at_50_A_ltx', 'rank_at_100_A_ltx', 'rank_at_k_B_ltx', 'NDCG_at_10_ltx', 'NDCG_at_20_ltx', 'NDCG_at_50_ltx', 'NDCG_at_100_ltx', 'POS_at_10_tf', 'POS_at_20_tf', 'POS_at_50_tf', 'POS_at_100_tf', 'NEG_at_10_tf', 'NEG_at_20_tf', 'NEG_at_50_tf', 'NEG_at_100_tf', 'users_DEL_tf', 'users_INS_tf', 'rank_at_10_A_tf', 'rank_at_20_A_tf', 'rank_at_50_A_tf', 'rank_at_100_A_tf', 'rank_at_k_B_tf', 'NDCG_at_10_tf', 'NDCG_at_20_tf', 'NDCG_at_50_tf', 'NDCG_at_100_tf']\n",
    "\n",
    "values = [POS_at_10_j_u, POS_at_20_j_u, POS_at_50_j_u, POS_at_100_j_u, NEG_at_10_j_u, NEG_at_20_j_u, NEG_at_50_j_u, NEG_at_100_j_u, users_DEL_j_u, users_INS_j_u,rank_at_10_A_j_u,rank_at_20_A_j_u, rank_at_50_A_j_u, rank_at_100_A_j_u, rank_at_k_B_j_u, NDCG_at_10_j_u, NDCG_at_20_j_u, NDCG_at_50_j_u, NDCG_at_100_j_u, POS_at_10_j_g, POS_at_20_j_g, POS_at_50_j_g, POS_at_100_j_g, NEG_at_10_j_g, NEG_at_20_j_g, NEG_at_50_j_g, NEG_at_100_j_g, users_DEL_j_g, users_INS_j_g, rank_at_10_A_j_g, rank_at_20_A_j_g, rank_at_50_A_j_g, rank_at_100_A_j_g, rank_at_k_B_j_g, NDCG_at_10_j_g, NDCG_at_20_j_g, NDCG_at_50_j_g, NDCG_at_100_j_g, POS_at_10_c_s, POS_at_20_c_s, POS_at_50_c_s, POS_at_100_c_s, NEG_at_10_c_s, NEG_at_20_c_s, NEG_at_50_c_s, NEG_at_100_c_s, users_DEL_c_s, users_INS_c_s, rank_at_10_A_c_s, rank_at_20_A_c_s, rank_at_50_A_c_s, rank_at_100_A_c_s, rank_at_k_B_c_s, NDCG_at_10_c_s, NDCG_at_20_c_s, NDCG_at_50_c_s, NDCG_at_100_c_s, POS_at_10_pop, POS_at_20_pop, POS_at_50_pop, POS_at_100_pop, NEG_at_10_pop, NEG_at_20_pop, NEG_at_50_pop, NEG_at_100_pop, users_DEL_pop, users_INS_pop, rank_at_10_A_pop, rank_at_20_A_pop, rank_at_50_A_pop, rank_at_100_A_pop, rank_at_k_B_pop, NDCG_at_10_pop, NDCG_at_20_pop, NDCG_at_50_pop, NDCG_at_100_pop, POS_at_10_lime, POS_at_20_lime, POS_at_50_lime, POS_at_100_lime, NEG_at_10_lime, NEG_at_20_lime, NEG_at_50_lime, NEG_at_100_lime, users_DEL_lime, users_INS_lime,rank_at_10_A_lime,rank_at_20_A_lime, rank_at_50_A_lime, rank_at_100_A_lime, rank_at_k_B_lime, NDCG_at_10_lime, NDCG_at_20_lime, NDCG_at_50_lime, NDCG_at_100_lime, POS_at_10_ltx, POS_at_20_ltx, POS_at_50_ltx, POS_at_100_ltx, NEG_at_10_ltx, NEG_at_20_ltx, NEG_at_50_ltx, NEG_at_100_ltx, users_DEL_ltx, users_INS_ltx, rank_at_10_A_ltx, rank_at_20_A_ltx, rank_at_50_A_ltx, rank_at_100_A_ltx, rank_at_k_B_ltx, NDCG_at_10_ltx, NDCG_at_20_ltx, NDCG_at_50_ltx, NDCG_at_100_ltx,POS_at_10_tf, POS_at_20_tf, POS_at_50_tf, POS_at_100_tf, NEG_at_10_tf, NEG_at_20_tf, NEG_at_50_tf, NEG_at_100_tf, users_DEL_tf, users_INS_tf, rank_at_10_A_tf, rank_at_20_A_tf, rank_at_50_A_tf, rank_at_100_A_tf, rank_at_k_B_tf, NDCG_at_10_tf, NDCG_at_20_tf, NDCG_at_50_tf, NDCG_at_100_tf]\n",
    "\n",
    "#names = ['POS_at_10_lime', 'POS_at_20_lime', 'POS_at_50_lime', 'POS_at_100_lime', 'NEG_at_10_lime', 'NEG_at_20_lime', 'NEG_at_50_lime', 'NEG_at_100_lime', 'users_DEL_lime', 'users_INS_lime', 'rank_at_10_A_lime', 'rank_at_20_A_lime', 'rank_at_50_A_lime', 'rank_at_100_A_lime', 'rank_at_k_B_lime', 'NDCG_at_10_lime', 'NDCG_at_20_lime', 'NDCG_at_50_lime', 'NDCG_at_100_lime']\n",
    "\n",
    "#values = [POS_at_10_lime, POS_at_20_lime, POS_at_50_lime, POS_at_100_lime, NEG_at_10_lime, NEG_at_20_lime, NEG_at_50_lime, NEG_at_100_lime, users_DEL_lime, users_INS_lime,rank_at_10_A_lime,rank_at_20_A_lime, rank_at_50_A_lime, rank_at_100_A_lime, rank_at_k_B_lime, NDCG_at_10_lime, NDCG_at_20_lime, NDCG_at_50_lime, NDCG_at_100_lime]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "MLP_results_ml1m = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "MLP_results_ml1m['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = MLP_results_ml1m['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "MLP_results_ml1m['AUC'] = AUC\n",
    "\n",
    "MLP_results_ml1m.head()\n",
    "\n",
    "MLP_results_ml1m.to_csv('MLP_18_4_results_yahoo_lime_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67ef58-2dbc-4d98-92e4-13d3fe6ac9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299568f-2605-461e-bd94-e6ac46d6f8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d335c2-5486-47fb-964e-35b3003ba557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a65167-6ea7-4049-ad5e-fbcb37a16207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed493c6c-fa39-4ce5-baf0-6dff7c087928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd229d7c-ccfc-40ce-8eb7-b9647e0f5aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01180e0-064e-4302-beb4-c648c46ad679",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572802e-a047-4231-93b6-1903934e32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_top_k(user_vector, item_id, item_tensor, num_of_bins, num_items, rec_model,model_combined, k=20,y_pred=None):\n",
    "    \n",
    "\n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    POS_masked[item_id] = 0\n",
    "    user_hist_size = np.sum(user_vector) - 1\n",
    "    item_vector = items_values_dict[item_id]\n",
    "\n",
    "    bins = [0] + [len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "\n",
    "    POS_at_20 = [0] * (len(bins))\n",
    "    total_items = 0\n",
    "\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            sim_items = find_LTX_mask(POS_masked, item_vector, item_id,model_combined)   \n",
    "             \n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "     \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked\n",
    "\n",
    "        POS_index = get_index_in_the_list(POS_masked,user_tensor, item_id, num_items, rec_model)+1\n",
    "                               \n",
    "        # for pos:\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "       \n",
    "        \n",
    "\n",
    "    res = np.array(POS_at_20)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d76825-7ed9-4031-ad39-4c540e06eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def my_objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.05)\n",
    "    #alpha_parameter = trial.suggest_float('alpha_parameter', 0.5,1.5)\n",
    "    alpha_parameter = trial.suggest_categorical('alpha_parameter', [0.1, 0.5, 1, 5, 10, 20])\n",
    "    #hidden_dim = trial.suggest_categorical('hidden_dim', [20, 64, 128])\n",
    "    epochs = trial.suggest_categorical('epochs', [5,10,20,30])\n",
    "\n",
    "    train_losses = []\n",
    "    hidden_dim = 20\n",
    "    num_of_bins = 10\n",
    "    num_of_rand_users = 50\n",
    "    POS_at_20_ltx = np.zeros(num_of_bins)\n",
    "\n",
    "    #Randomly sampled num_of_rand_users users from test array\n",
    "    #random_rows = np.random.choice(test_array.shape[0], num_of_rand_users, replace=False)\n",
    "    random_sampled_array = test_array[:500]\n",
    "\n",
    "    rec_model = Recommender_G(num_items, hidden_dim)\n",
    "    rec_model.load_state_dict(torch.load('recommender_model_yahoo.pt'))\n",
    "    for param in rec_model.parameters():\n",
    "        param.requires_grad= False\n",
    "\n",
    "    explainer_model_g = Explainer_G(rec_model, num_items, hidden_dim).to(device) \n",
    "    model_combined = LossModelCombined(alpha_parameter, rec_model, explainer_model_g, hidden_dim).to(device)\n",
    "    optimizer_comb = torch.optim.Adam(model_combined.parameters(), lr)\n",
    "        \n",
    "    #Train LTX\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        model_combined.train()\n",
    "        explainer_model_g.train()\n",
    "        for i in range(train_unique_arr.shape[0]):\n",
    "                \n",
    "            #user data\n",
    "            user_id = train_unique_arr[i][-3]\n",
    "            user_vector = train_unique_arr[i][:-3]\n",
    "            #get top1 of this user for LTX training\n",
    "                \n",
    "            top1_item = np.argmax(topk_train[user_id])\n",
    "\n",
    "            item_vector = items_values_dict[top1_item]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "                \n",
    "            user_vector[top1_item] = 0 \n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "                \n",
    "            #train model\n",
    "            optimizer_comb.zero_grad()\n",
    "            x_masked_g, loss_comb_g = model_combined(user_tensor, item_tensor)\n",
    "                \n",
    "            train_loss+=loss_comb_g.item()\n",
    "                    \n",
    "            loss_comb_g.backward()\n",
    "            optimizer_comb.step()\n",
    "          \n",
    "        train_losses.append(train_loss/train_unique_arr.shape[0])\n",
    "      \n",
    "        #print(f\"Epoch {epoch}, Train Loss {train_loss/train_unique_arr.shape[0]:.4f}\")\n",
    "            \n",
    "    #Monitoring on POS metric after each epoch\n",
    "    model_combined.eval()\n",
    "    explainer_model_g.eval()\n",
    "    for j in range(random_sampled_array.shape[0]):\n",
    "                \n",
    "        user_id = random_sampled_array[j][-2]\n",
    "        user_vector = random_sampled_array[j][:-2]\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        #get top1 of this test user item\n",
    "        top1_item_test = np.argmax(topk_test[user_id])\n",
    "                \n",
    "        item_vector = items_values_dict[top1_item_test]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "                \n",
    "        user_vector[top1_item_test] = 0 \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "                \n",
    "        res = calculate_pos_top_k(user_vector, top1_item_test, item_tensor, num_of_bins, num_items, rec_model, model_combined, k=20,y_pred=None)\n",
    "\n",
    "        POS_at_20_ltx += res       \n",
    "    #print(\"POS@20 at epoch {:d} is {:.4f} \".format(int(epoch), np.mean(POS_at_20_ltx)/(j+1)))\n",
    "    return np.mean(POS_at_20_ltx)/j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d73c9-89bc-4a21-9f58-85dab3a61e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(my_objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters and corresponding metric value\n",
    "print(\"Best hyperparameters: {}\".format(study.best_params))\n",
    "print(\"Best metric value: {}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d4ed3-9b2e-4a26-bbfa-cabc52bf0f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e7ddd-e7f1-44b3-8af6-aeab39f76b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
